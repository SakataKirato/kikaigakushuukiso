{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "id": "w0miHB_4j6fO",
    "outputId": "2b8535a0-4018-4af3-c73c-e0f06916eba3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install schedulefree -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:35.009259Z",
     "iopub.status.busy": "2024-10-31T16:06:35.008896Z",
     "iopub.status.idle": "2024-10-31T16:06:35.040481Z",
     "shell.execute_reply": "2024-10-31T16:06:35.039428Z",
     "shell.execute_reply.started": "2024-10-31T16:06:35.009221Z"
    },
    "id": "wmcUA-Kyj6fR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
    "sub=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:35.160063Z",
     "iopub.status.busy": "2024-10-31T16:06:35.159559Z",
     "iopub.status.idle": "2024-10-31T16:06:38.637873Z",
     "shell.execute_reply": "2024-10-31T16:06:38.636808Z",
     "shell.execute_reply.started": "2024-10-31T16:06:35.160004Z"
    },
    "id": "2SYojsJUj6fS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Drop unneeded columns and handle missing values\n",
    "train = train.drop(columns=[\"Unnamed: 12\", \"id\"])  # Dropping unnecessary columns\n",
    "\n",
    "# CRITICAL FIX: Rename TA1.x to TA1 to match test data\n",
    "train = train.rename(columns={\"TA1.x\": \"TA1\"})\n",
    "\n",
    "# ============================================\n",
    "# Extract Raw Features and Target\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA PREPARATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Extract features and target\n",
    "feature_columns = [col for col in train.columns if col != 'DIC']\n",
    "X_raw = train[feature_columns].copy()\n",
    "y_raw = train['DIC'].values.copy()\n",
    "X_test_raw = test[feature_columns].copy()\n",
    "\n",
    "print(f\"元の訓練データ: {X_raw.shape}\")\n",
    "print(f\"テストデータ: {X_test_raw.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# Holdout Validation Setup\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"HOLDOUT VALIDATION SETUP\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 80% train, 20% validation\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train_raw.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val_raw.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_raw.shape[0]} samples\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:38.679793Z",
     "iopub.status.busy": "2024-10-31T16:06:38.679208Z",
     "iopub.status.idle": "2024-10-31T16:06:38.702210Z",
     "shell.execute_reply": "2024-10-31T16:06:38.701153Z",
     "shell.execute_reply.started": "2024-10-31T16:06:38.679681Z"
    },
    "id": "8kUw3FU7j6fT",
    "outputId": "a9af3f6d-335d-4a85-c2c1-bf760562a6b5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ResNet Model for Tabular Data\n",
    "# Based on \"Revisiting Deep Learning Models for Tabular Data\" (NeurIPS 2021)\n",
    "# ============================================\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Block for tabular data\n",
    "    ResNetBlock(x) = x + Dropout(Linear(Dropout(ReLU(Linear(BatchNorm(x))))))\n",
    "    \"\"\"\n",
    "    def __init__(self, d, hidden_factor=2, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d: Dimension of input and output\n",
    "            hidden_factor: Factor to determine hidden layer size (hidden = d * hidden_factor)\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        \n",
    "        hidden_dim = int(d * hidden_factor)\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d(d)\n",
    "        self.linear1 = nn.Linear(d, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(hidden_dim, d)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_normal_(self.linear1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.linear2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.linear1.bias, 0)\n",
    "        nn.init.constant_(self.linear2.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Main path: BatchNorm -> Linear -> ReLU -> Dropout -> Linear -> Dropout\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        return residual + x\n",
    "\n",
    "\n",
    "class ResNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet for tabular data\n",
    "    ResNet(x) = Prediction(ResNetBlock(...(ResNetBlock(Linear(x)))))\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, d=256, n_blocks=4, hidden_factor=2, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            d: Dimension of ResNet blocks\n",
    "            n_blocks: Number of ResNet blocks\n",
    "            hidden_factor: Hidden layer factor for each block\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super(ResNetModel, self).__init__()\n",
    "        \n",
    "        # Initial projection\n",
    "        self.input_layer = nn.Linear(input_size, d)\n",
    "        nn.init.kaiming_normal_(self.input_layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.input_layer.bias, 0)\n",
    "        \n",
    "        # ResNet blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResNetBlock(d, hidden_factor, dropout_rate) \n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Prediction head: BatchNorm -> ReLU -> Linear\n",
    "        self.final_norm = nn.BatchNorm1d(d)\n",
    "        self.final_relu = nn.ReLU()\n",
    "        self.output = nn.Linear(d, 1)\n",
    "        nn.init.xavier_normal_(self.output.weight)\n",
    "        nn.init.constant_(self.output.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial projection\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Pass through ResNet blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Prediction\n",
    "        x = self.final_norm(x)\n",
    "        x = self.final_relu(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.optim as optim\nfrom schedulefree import RAdamScheduleFree, AdamWScheduleFree\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch.amp import autocast, GradScaler\nfrom sklearn.preprocessing import RobustScaler\nimport copy\nimport optuna\n\n# ============================================\n# Configuration\n# ============================================\nUSE_OPTUNA = True  # Set to True to use Optuna hyperparameter search\nN_TRIALS = 100  # Number of Optuna trials\nTIMEOUT = None  # Time limit for Optuna in seconds (None = no limit)\nOPTUNA_SEED = 42  # Fixed seed for Optuna search (single seed for reproducibility)\n\n# Fixed settings (not tuned by Optuna)\nuse_c_mixup = True\nc_mixup_alpha = 1.0\nc_mixup_sigma = 1.0\nc_mixup_factor = 2\nepochs = 100000\nearly_stopping_patience = 500\n\n# Multiple seed training (after Optuna search)\nnum_seeds = 5\nstart_seed = 1000\n\nprint(\"=\"*60)\nprint(\"ResNet for Tabular Data - Optuna Hyperparameter Search\")\nprint(\"=\"*60)\nprint(f\"USE_OPTUNA: {USE_OPTUNA}\")\nif USE_OPTUNA:\n    print(f\"N_TRIALS: {N_TRIALS}\")\n    print(f\"TIMEOUT: {TIMEOUT}\")\n    print(f\"OPTUNA_SEED: {OPTUNA_SEED} (fixed for search)\")\nprint(f\"epochs: {epochs}\")\nprint(f\"early_stopping_patience: {early_stopping_patience}\")\nprint(f\"num_seeds: {num_seeds} (seeds {start_seed} to {start_seed + num_seeds - 1})\")\nprint(\"=\"*60)\n\n\nclass EMA:\n    \"\"\"Exponential Moving Average (EMA) for model weights\"\"\"\n    def __init__(self, model, decay=0.999):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        self.register()\n    \n    def register(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n    \n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_average = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data\n                self.shadow[name] = new_average.clone()\n    \n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data.clone()\n                param.data = self.shadow[name]\n    \n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n        self.backup = {}\n\n\ndef smooth_clip(x, clip_val=3.0):\n    \"\"\"Apply smooth clipping using tanh\"\"\"\n    return np.tanh(x / clip_val) * clip_val\n\n\ndef inverse_smooth_clip(x, clip_val=3.0):\n    \"\"\"Inverse of smooth clipping with numerical stability\"\"\"\n    x_normalized = x / clip_val\n    # Use more conservative clipping to avoid numerical instability in arctanh\n    # arctanh(x) -> infinity as x -> ±1, so clip to ±0.995 for safety\n    x_safe = np.clip(x_normalized, -0.995, 0.995)\n    result = np.arctanh(x_safe) * clip_val\n    # Final safety check: replace any remaining inf/nan with bounded values\n    result = np.where(np.isfinite(result), result, np.sign(x) * clip_val * 10)\n    return result\n\n\ndef c_mixup(X, y, alpha=1.0, sigma=1.0, augment_factor=2):\n    \"\"\"C-Mixup (Calibrated Mixup) data augmentation\"\"\"\n    n_samples = X.shape[0]\n    \n    y_expanded = y.reshape(-1, 1)\n    label_distances = (y_expanded - y_expanded.T) ** 2\n    \n    sampling_probs = np.exp(-label_distances / (2 * sigma ** 2))\n    np.fill_diagonal(sampling_probs, 0)\n    row_sums = sampling_probs.sum(axis=1, keepdims=True)\n    row_sums[row_sums == 0] = 1\n    sampling_probs = sampling_probs / row_sums\n    \n    X_augmented = []\n    y_augmented = []\n    \n    for _ in range(augment_factor):\n        for i in range(n_samples):\n            j = np.random.choice(n_samples, p=sampling_probs[i])\n            lambda_mix = np.random.beta(alpha, alpha)\n            \n            x_mix = lambda_mix * X[i] + (1 - lambda_mix) * X[j]\n            y_mix = lambda_mix * y[i] + (1 - lambda_mix) * y[j]\n            \n            X_augmented.append(x_mix)\n            y_augmented.append(y_mix)\n    \n    X_aug = np.vstack([X] + [np.array(X_augmented)])\n    y_aug = np.hstack([y] + [np.array(y_augmented)])\n    \n    return X_aug, y_aug\n\n\ndef train_and_evaluate(params, X_train_raw, X_val_raw, y_train_raw, y_val_raw, seed, verbose=False):\n    \"\"\"\n    Train a single model with given hyperparameters and return validation RMSE\n    \"\"\"\n    # Set random seeds\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    # Preprocessing: RobustScaler\n    scaler_X = RobustScaler()\n    scaler_y = RobustScaler()\n    \n    X_train_scaled = scaler_X.fit_transform(X_train_raw.values)\n    X_val_scaled = scaler_X.transform(X_val_raw.values)\n    \n    y_train_scaled = scaler_y.fit_transform(y_train_raw.reshape(-1, 1)).flatten()\n    y_val_scaled = scaler_y.transform(y_val_raw.reshape(-1, 1)).flatten()\n    \n    # Smooth Clipping\n    clip_val = 3.0\n    X_train_clipped = smooth_clip(X_train_scaled, clip_val=clip_val)\n    X_val_clipped = smooth_clip(X_val_scaled, clip_val=clip_val)\n    y_train_clipped = smooth_clip(y_train_scaled, clip_val=clip_val)\n    y_val_clipped = smooth_clip(y_val_scaled, clip_val=clip_val)\n    \n    # C-Mixup augmentation\n    if use_c_mixup:\n        X_train_final, y_train_final = c_mixup(\n            X_train_clipped, \n            y_train_clipped, \n            alpha=c_mixup_alpha, \n            sigma=c_mixup_sigma,\n            augment_factor=c_mixup_factor\n        )\n    else:\n        X_train_final = X_train_clipped\n        y_train_final = y_train_clipped\n    \n    X_val_final = X_val_clipped\n    y_val_final = y_val_clipped\n    \n    # Create DataLoaders\n    train_dataset = TensorDataset(\n        torch.tensor(X_train_final, dtype=torch.float32), \n        torch.tensor(y_train_final, dtype=torch.float32)\n    )\n    val_dataset = TensorDataset(\n        torch.tensor(X_val_final, dtype=torch.float32), \n        torch.tensor(y_val_final, dtype=torch.float32)\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False, pin_memory=True)\n    \n    # Initialize Model\n    model = ResNetModel(\n        input_size=X_train_final.shape[1],\n        d=params['d'],\n        n_blocks=params['n_blocks'],\n        hidden_factor=params['hidden_factor'],\n        dropout_rate=params['residual_dropout']\n    )\n    model = model.to(device)\n    \n    # Update hidden_dropout in each block\n    for block in model.blocks:\n        block.dropout1 = nn.Dropout(params['hidden_dropout'])\n    \n    # Initialize EMA (conditional based on params)\n    use_ema = params['use_ema']\n    ema_decay = params.get('ema_decay', 0.999)\n    ema = EMA(model, decay=ema_decay) if use_ema else None\n    \n    # Loss function\n    loss_map = {\n        'mse': nn.MSELoss(), \n        'mae': nn.L1Loss(), \n        'smooth_l1': nn.SmoothL1Loss(), \n        'huber': nn.HuberLoss()\n    }\n    criterion = loss_map[params['loss_function']]\n    \n    # Initialize Optimizer\n    optimizer_name = params['optimizer']\n    lr = params['learning_rate']\n    weight_decay = params['weight_decay']\n    \n    is_schedulefree = optimizer_name.endswith('_schedulefree')\n    if optimizer_name == 'adamw_schedulefree':\n        optimizer = AdamWScheduleFree(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optimizer_name == 'radam_schedulefree':\n        optimizer = RAdamScheduleFree(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optimizer_name == 'adamw':\n        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n    \n    # Mixed precision training\n    use_amp = torch.cuda.is_available()\n    scaler = GradScaler('cuda') if use_amp else None\n    \n    # Training Loop\n    best_val_rmse = float('inf')\n    best_model_state = None\n    best_ema_shadow = None\n    patience_counter = 0\n    \n    for epoch in range(epochs):\n        # Training mode\n        if is_schedulefree:\n            optimizer.train()\n        \n        model.train()\n        \n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            \n            optimizer.zero_grad()\n            \n            if use_amp:\n                with autocast('cuda'):\n                    outputs = model(X_batch)\n                    loss = criterion(outputs.squeeze(-1), y_batch)\n                \n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(X_batch)\n                loss = criterion(outputs.squeeze(-1), y_batch)\n                loss.backward()\n                optimizer.step()\n            \n            if ema is not None:\n                ema.update()\n        \n        # Evaluation mode\n        if is_schedulefree:\n            optimizer.eval()\n        \n        if ema is not None:\n            ema.apply_shadow()\n        \n        model.eval()\n        \n        # Validation\n        val_predictions = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                \n                if use_amp:\n                    with autocast('cuda'):\n                        outputs = model(X_batch)\n                else:\n                    outputs = model(X_batch)\n                \n                val_predictions.extend(outputs.squeeze(-1).cpu().numpy())\n                val_targets.extend(y_batch.cpu().numpy())\n        \n        if ema is not None:\n            ema.restore()\n        \n        # Calculate RMSE in original scale\n        # Inverse smooth clipping\n        val_predictions_unclipped = inverse_smooth_clip(np.array(val_predictions), clip_val=clip_val)\n        val_targets_unclipped = inverse_smooth_clip(np.array(val_targets), clip_val=clip_val)\n        \n        # Inverse scaling\n        val_predictions_original = scaler_y.inverse_transform(val_predictions_unclipped.reshape(-1, 1)).flatten()\n        val_targets_original = scaler_y.inverse_transform(val_targets_unclipped.reshape(-1, 1)).flatten()\n        \n        val_rmse = np.sqrt(np.mean((val_predictions_original - val_targets_original)**2))\n        \n        # Print progress every 10 epochs if verbose\n        if verbose and (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch + 1}/{epochs} - val_RMSE: {val_rmse:.4f} (best: {best_val_rmse:.4f})\")\n        \n        # Early stopping\n        if val_rmse < best_val_rmse:\n            best_val_rmse = val_rmse\n            best_model_state = copy.deepcopy(model.state_dict())\n            if ema is not None:\n                best_ema_shadow = copy.deepcopy(ema.shadow)\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= early_stopping_patience:\n            if verbose:\n                print(f\"Early stopping at epoch {epoch + 1}\")\n            break\n    \n    return best_val_rmse\n\n\ndef objective(trial):\n    \"\"\"\n    Optuna objective function\n    Uses OPTUNA_SEED for reproducible hyperparameter search\n    \"\"\"\n    # Hyperparameters to tune\n    params = {}\n    \n    # Model architecture\n    params['d'] = trial.suggest_int('d', 64, 512)\n    params['n_blocks'] = trial.suggest_int('n_blocks', 1, 8)\n    params['hidden_factor'] = trial.suggest_float('hidden_factor', 1.0, 4.0)\n    params['hidden_dropout'] = trial.suggest_float('hidden_dropout', 0.0, 0.5)\n    \n    # Residual dropout (conditional: 0 or 0~0.5)\n    use_residual_dropout = trial.suggest_categorical('use_residual_dropout', [True, False])\n    if use_residual_dropout:\n        params['residual_dropout'] = trial.suggest_float('residual_dropout', 0.0, 0.5)\n    else:\n        params['residual_dropout'] = 0.0\n    \n    # Training hyperparameters\n    params['learning_rate'] = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    \n    # Weight decay (conditional: 0 or 1e-6~1e-3)\n    use_weight_decay = trial.suggest_categorical('use_weight_decay', [True, False])\n    if use_weight_decay:\n        params['weight_decay'] = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n    else:\n        params['weight_decay'] = 0.0\n    \n    params['batch_size'] = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n    params['optimizer'] = trial.suggest_categorical('optimizer', ['adamw', 'adamw_schedulefree', 'radam_schedulefree'])\n    params['loss_function'] = trial.suggest_categorical('loss_function', ['mse', 'mae', 'smooth_l1', 'huber'])\n    \n    # EMA (conditional: use or not, and if use, decay value)\n    params['use_ema'] = trial.suggest_categorical('use_ema', [True, False])\n    if params['use_ema']:\n        params['ema_decay'] = trial.suggest_float('ema_decay', 0.99, 0.9999)\n    else:\n        params['ema_decay'] = 0.999  # default (not used)\n    \n    # Train with FIXED seed for Optuna search (reproducibility)\n    val_rmse = train_and_evaluate(params, X_train_raw, X_val_raw, y_train_raw, y_val_raw, seed=OPTUNA_SEED, verbose=False)\n    \n    return val_rmse\n\n\n# ============================================\n# OPTUNA HYPERPARAMETER SEARCH\n# ============================================\nif USE_OPTUNA:\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING OPTUNA HYPERPARAMETER SEARCH\")\n    print(f\"Using fixed seed: {OPTUNA_SEED}\")\n    print(\"=\"*60)\n    \n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"OPTUNA SEARCH COMPLETE\")\n    print(\"=\"*60)\n    print(f\"Best trial:\")\n    print(f\"  Value (val_RMSE): {study.best_trial.value:.4f}\")\n    print(f\"  Params:\")\n    for key, value in study.best_trial.params.items():\n        print(f\"    {key}: {value}\")\n    print(\"=\"*60)\n    \n    # Extract best hyperparameters\n    best_params = {}\n    best_params['d'] = study.best_trial.params['d']\n    best_params['n_blocks'] = study.best_trial.params['n_blocks']\n    best_params['hidden_factor'] = study.best_trial.params['hidden_factor']\n    best_params['hidden_dropout'] = study.best_trial.params['hidden_dropout']\n    \n    # Handle conditional residual_dropout\n    if study.best_trial.params.get('use_residual_dropout', False):\n        best_params['residual_dropout'] = study.best_trial.params.get('residual_dropout', 0.0)\n    else:\n        best_params['residual_dropout'] = 0.0\n    \n    best_params['learning_rate'] = study.best_trial.params['learning_rate']\n    \n    # Handle conditional weight_decay\n    if study.best_trial.params.get('use_weight_decay', False):\n        best_params['weight_decay'] = study.best_trial.params.get('weight_decay', 0.0)\n    else:\n        best_params['weight_decay'] = 0.0\n    \n    best_params['batch_size'] = study.best_trial.params['batch_size']\n    best_params['optimizer'] = study.best_trial.params['optimizer']\n    best_params['loss_function'] = study.best_trial.params['loss_function']\n    \n    # Handle conditional EMA\n    best_params['use_ema'] = study.best_trial.params['use_ema']\n    if best_params['use_ema']:\n        best_params['ema_decay'] = study.best_trial.params.get('ema_decay', 0.999)\n    else:\n        best_params['ema_decay'] = 0.999  # default (not used)\n    \nelse:\n    # Default hyperparameters (if not using Optuna)\n    best_params = {\n        'd': 256,\n        'n_blocks': 4,\n        'hidden_factor': 2.0,\n        'hidden_dropout': 0.1,\n        'residual_dropout': 0.1,\n        'learning_rate': 1e-3,\n        'weight_decay': 1e-4,\n        'batch_size': 64,\n        'optimizer': 'adamw_schedulefree',\n        'loss_function': 'mae',\n        'use_ema': True,\n        'ema_decay': 0.999,\n    }\n    print(\"\\nUsing default hyperparameters (Optuna disabled)\")\n\n\n# ============================================\n# TRAIN WITH BEST HYPERPARAMETERS (MULTIPLE SEEDS)\n# ============================================\nprint(\"\\n\" + \"#\"*60)\nprint(\"# TRAINING WITH BEST HYPERPARAMETERS (MULTIPLE SEEDS)\")\nprint(\"#\"*60)\nprint(f\"\\nBest hyperparameters:\")\nfor key, value in best_params.items():\n    print(f\"  {key}: {value}\")\nprint()\n\nbest_rmses = []\n\nfor seed_idx in range(num_seeds):\n    SEED = start_seed + seed_idx\n    \n    print(f\"\\n{'#'*60}\")\n    print(f\"# SEED {SEED} ({seed_idx + 1}/{num_seeds})\")\n    print(f\"{'#'*60}\\n\")\n    \n    val_rmse = train_and_evaluate(best_params, X_train_raw, X_val_raw, y_train_raw, y_val_raw, SEED, verbose=True)\n    best_rmses.append(val_rmse)\n    \n    print(f\"\\nValidation RMSE: {val_rmse:.4f}\")\n    \n    # Train on full data and predict on test\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    \n    # Preprocessing\n    scaler_X = RobustScaler()\n    scaler_y = RobustScaler()\n    \n    X_train_scaled = scaler_X.fit_transform(X_train_raw.values)\n    X_test_scaled = scaler_X.transform(X_test_raw.values)\n    \n    y_train_scaled = scaler_y.fit_transform(y_train_raw.reshape(-1, 1)).flatten()\n    \n    # Smooth Clipping\n    clip_val = 3.0\n    X_train_clipped = smooth_clip(X_train_scaled, clip_val=clip_val)\n    X_test_clipped = smooth_clip(X_test_scaled, clip_val=clip_val)\n    y_train_clipped = smooth_clip(y_train_scaled, clip_val=clip_val)\n    \n    if use_c_mixup:\n        X_train_final, y_train_final = c_mixup(\n            X_train_clipped, \n            y_train_clipped, \n            alpha=c_mixup_alpha, \n            sigma=c_mixup_sigma,\n            augment_factor=c_mixup_factor\n        )\n    else:\n        X_train_final = X_train_clipped\n        y_train_final = y_train_clipped\n    \n    # Create DataLoader\n    train_dataset = TensorDataset(\n        torch.tensor(X_train_final, dtype=torch.float32), \n        torch.tensor(y_train_final, dtype=torch.float32)\n    )\n    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, pin_memory=True)\n    \n    # Initialize Model\n    model = ResNetModel(\n        input_size=X_train_final.shape[1],\n        d=best_params['d'],\n        n_blocks=best_params['n_blocks'],\n        hidden_factor=best_params['hidden_factor'],\n        dropout_rate=best_params['residual_dropout']\n    )\n    model = model.to(device)\n    \n    for block in model.blocks:\n        block.dropout1 = nn.Dropout(best_params['hidden_dropout'])\n    \n    # Initialize EMA and Optimizer\n    use_ema = best_params['use_ema']\n    ema_decay = best_params['ema_decay']\n    ema = EMA(model, decay=ema_decay) if use_ema else None\n    \n    loss_map = {\n        'mse': nn.MSELoss(), \n        'mae': nn.L1Loss(), \n        'smooth_l1': nn.SmoothL1Loss(), \n        'huber': nn.HuberLoss()\n    }\n    criterion = loss_map[best_params['loss_function']]\n    \n    optimizer_name = best_params['optimizer']\n    is_schedulefree = optimizer_name.endswith('_schedulefree')\n    if optimizer_name == 'adamw_schedulefree':\n        optimizer = AdamWScheduleFree(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n    elif optimizer_name == 'radam_schedulefree':\n        optimizer = RAdamScheduleFree(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n    else:\n        optimizer = optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n    \n    use_amp = torch.cuda.is_available()\n    scaler_grad = GradScaler('cuda') if use_amp else None\n    \n    # Train\n    print(\"Training final model...\")\n    for epoch in range(epochs):\n        if is_schedulefree:\n            optimizer.train()\n        \n        model.train()\n        \n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            \n            optimizer.zero_grad()\n            \n            if use_amp:\n                with autocast('cuda'):\n                    outputs = model(X_batch)\n                    loss = criterion(outputs.squeeze(-1), y_batch)\n                \n                scaler_grad.scale(loss).backward()\n                scaler_grad.step(optimizer)\n                scaler_grad.update()\n            else:\n                outputs = model(X_batch)\n                loss = criterion(outputs.squeeze(-1), y_batch)\n                loss.backward()\n                optimizer.step()\n            \n            if ema is not None:\n                ema.update()\n    \n    if ema is not None:\n        ema.apply_shadow()\n    \n    # Predict\n    model.eval()\n    test_tensor = torch.tensor(X_test_clipped, dtype=torch.float32).to(device)\n    \n    with torch.no_grad():\n        if use_amp:\n            with autocast('cuda'):\n                predictions_clipped = model(test_tensor).squeeze().cpu().numpy()\n        else:\n            predictions_clipped = model(test_tensor).squeeze().cpu().numpy()\n        \n        # Inverse smooth clipping\n        predictions_unclipped = inverse_smooth_clip(predictions_clipped, clip_val=clip_val)\n        \n        # Inverse scaling\n        predictions = scaler_y.inverse_transform(predictions_unclipped.reshape(-1, 1)).flatten()\n    \n    # Save submission\n    submission = pd.DataFrame({\n        \"id\": range(1455, 1455 + len(predictions)), \n        \"DIC\": predictions\n    })\n    submission_filename = f\"submission_resnet_seed_{SEED}.csv\"\n    submission.to_csv(submission_filename, index=False)\n    \n    print(f\"Saved: {submission_filename}\")\n\nprint(f\"\\n{'#'*60}\")\nprint(f\"# ALL SEEDS COMPLETE\")\nprint(f\"{'#'*60}\")\nprint(f\"Validation RMSEs: {best_rmses}\")\nprint(f\"Mean RMSE: {np.mean(best_rmses):.4f} ± {np.std(best_rmses):.4f}\")\nprint(f\"{'#'*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# name: 坂田煌翔\n",
    "# student_id: 62408940"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7941427,
     "sourceId": 49552,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}