{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "id": "w0miHB_4j6fO",
    "outputId": "2b8535a0-4018-4af3-c73c-e0f06916eba3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install schedulefree -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:35.009259Z",
     "iopub.status.busy": "2024-10-31T16:06:35.008896Z",
     "iopub.status.idle": "2024-10-31T16:06:35.040481Z",
     "shell.execute_reply": "2024-10-31T16:06:35.039428Z",
     "shell.execute_reply.started": "2024-10-31T16:06:35.009221Z"
    },
    "id": "wmcUA-Kyj6fR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
    "sub=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:35.160063Z",
     "iopub.status.busy": "2024-10-31T16:06:35.159559Z",
     "iopub.status.idle": "2024-10-31T16:06:38.637873Z",
     "shell.execute_reply": "2024-10-31T16:06:38.636808Z",
     "shell.execute_reply.started": "2024-10-31T16:06:35.160004Z"
    },
    "id": "2SYojsJUj6fS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Drop unneeded columns and handle missing values\n",
    "train = train.drop(columns=[\"Unnamed: 12\", \"id\"])  # Dropping unnecessary columns\n",
    "\n",
    "# CRITICAL FIX: Rename TA1.x to TA1 to match test data\n",
    "train = train.rename(columns={\"TA1.x\": \"TA1\"})\n",
    "\n",
    "# ============================================\n",
    "# Extract Raw Features and Target\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA PREPARATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Extract features and target\n",
    "feature_columns = [col for col in train.columns if col != 'DIC']\n",
    "X_raw = train[feature_columns].copy()\n",
    "y_raw = train['DIC'].values.copy()\n",
    "X_test_raw = test[feature_columns].copy()\n",
    "\n",
    "print(f\"元の訓練データ: {X_raw.shape}\")\n",
    "print(f\"テストデータ: {X_test_raw.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# Holdout Validation Setup\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"HOLDOUT VALIDATION SETUP\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 80% train, 20% validation\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train_raw.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val_raw.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_raw.shape[0]} samples\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:38.679793Z",
     "iopub.status.busy": "2024-10-31T16:06:38.679208Z",
     "iopub.status.idle": "2024-10-31T16:06:38.702210Z",
     "shell.execute_reply": "2024-10-31T16:06:38.701153Z",
     "shell.execute_reply.started": "2024-10-31T16:06:38.679681Z"
    },
    "id": "8kUw3FU7j6fT",
    "outputId": "a9af3f6d-335d-4a85-c2c1-bf760562a6b5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ResNet Model for Tabular Data\n",
    "# Based on \"Revisiting Deep Learning Models for Tabular Data\" (NeurIPS 2021)\n",
    "# ============================================\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Block for tabular data\n",
    "    ResNetBlock(x) = x + Dropout(Linear(Dropout(ReLU(Linear(BatchNorm(x))))))\n",
    "    \"\"\"\n",
    "    def __init__(self, d, hidden_factor=2, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d: Dimension of input and output\n",
    "            hidden_factor: Factor to determine hidden layer size (hidden = d * hidden_factor)\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        \n",
    "        hidden_dim = int(d * hidden_factor)\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d(d)\n",
    "        self.linear1 = nn.Linear(d, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(hidden_dim, d)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_normal_(self.linear1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.linear2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.linear1.bias, 0)\n",
    "        nn.init.constant_(self.linear2.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Main path: BatchNorm -> Linear -> ReLU -> Dropout -> Linear -> Dropout\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        return residual + x\n",
    "\n",
    "\n",
    "class ResNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet for tabular data\n",
    "    ResNet(x) = Prediction(ResNetBlock(...(ResNetBlock(Linear(x)))))\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, d=256, n_blocks=4, hidden_factor=2, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            d: Dimension of ResNet blocks\n",
    "            n_blocks: Number of ResNet blocks\n",
    "            hidden_factor: Hidden layer factor for each block\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super(ResNetModel, self).__init__()\n",
    "        \n",
    "        # Initial projection\n",
    "        self.input_layer = nn.Linear(input_size, d)\n",
    "        nn.init.kaiming_normal_(self.input_layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.input_layer.bias, 0)\n",
    "        \n",
    "        # ResNet blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResNetBlock(d, hidden_factor, dropout_rate) \n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Prediction head: BatchNorm -> ReLU -> Linear\n",
    "        self.final_norm = nn.BatchNorm1d(d)\n",
    "        self.final_relu = nn.ReLU()\n",
    "        self.output = nn.Linear(d, 1)\n",
    "        nn.init.xavier_normal_(self.output.weight)\n",
    "        nn.init.constant_(self.output.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial projection\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Pass through ResNet blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Prediction\n",
    "        x = self.final_norm(x)\n",
    "        x = self.final_relu(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Denoising Autoencoder (DAE) for Tabular Data\n",
    "# ResNetBlockベースの構造で統一\n",
    "# ============================================\n",
    "\n",
    "class DAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DAE Encoder using ResNetBlock structure\n",
    "    ResNetBlockと完全に同じ構造で、重みの転用を容易にする\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, d, n_blocks, hidden_factor=2, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            d: Dimension of ResNet blocks\n",
    "            n_blocks: Number of ResNet blocks\n",
    "            hidden_factor: Hidden layer factor for each block\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super(DAEEncoder, self).__init__()\n",
    "        \n",
    "        # Initial projection (ResNetと同じ)\n",
    "        self.input_layer = nn.Linear(input_size, d)\n",
    "        nn.init.kaiming_normal_(self.input_layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.input_layer.bias, 0)\n",
    "        \n",
    "        # ResNet blocks (ResNetと完全に同じ構造)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResNetBlock(d, hidden_factor, dropout_rate) \n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Final normalization (エンコーダーの出力を正規化)\n",
    "        self.final_norm = nn.BatchNorm1d(d)\n",
    "        self.final_relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial projection\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Pass through ResNet blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.final_norm(x)\n",
    "        x = self.final_relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DAEDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DAE Decoder to reconstruct input from latent representation\n",
    "    \"\"\"\n",
    "    def __init__(self, d, output_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d: Dimension of latent representation\n",
    "            output_size: Number of output features (same as input)\n",
    "        \"\"\"\n",
    "        super(DAEDecoder, self).__init__()\n",
    "        \n",
    "        # Simple decoder: Linear projection back to input space\n",
    "        self.output_layer = nn.Linear(d, output_size)\n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "        nn.init.constant_(self.output_layer.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Denoising Autoencoder with ResNetBlock-based encoder\n",
    "    エンコーダーがResNetと完全に同じ構造なので、重みの転用が容易\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, d=256, n_blocks=4, hidden_factor=2, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            d: Dimension of ResNet blocks\n",
    "            n_blocks: Number of ResNet blocks\n",
    "            hidden_factor: Hidden layer factor for each block\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = DAEEncoder(input_size, d, n_blocks, hidden_factor, dropout_rate)\n",
    "        self.decoder = DAEDecoder(d, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Run only the encoder part\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class ResNetWithDAE(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet model with DAE pretrained encoder\n",
    "    DAEのエンコーダー全体（input_layer + blocks）をそのまま使用\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, d=256, n_blocks=4, hidden_factor=2, \n",
    "                 hidden_dropout=0.1, residual_dropout=0.1,\n",
    "                 use_dae_encoder=False, n_additional_blocks=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            d: Dimension of ResNet blocks\n",
    "            n_blocks: Number of blocks in DAE encoder (事前学習済み)\n",
    "            hidden_factor: Hidden layer factor for each block\n",
    "            hidden_dropout: Hidden layer dropout (used in additional blocks)\n",
    "            residual_dropout: Residual dropout (used in DAE encoder blocks)\n",
    "            use_dae_encoder: If True, use pretrained DAE encoder\n",
    "            n_additional_blocks: Number of additional ResNet blocks after DAE encoder\n",
    "        \"\"\"\n",
    "        super(ResNetWithDAE, self).__init__()\n",
    "        \n",
    "        self.use_dae_encoder = use_dae_encoder\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_additional_blocks = n_additional_blocks\n",
    "        \n",
    "        if use_dae_encoder:\n",
    "            # DAE encoder (will be loaded from pretrained weights)\n",
    "            self.dae_encoder = DAEEncoder(input_size, d, n_blocks, hidden_factor, residual_dropout)\n",
    "            \n",
    "            # Additional ResNet blocks (optional, for further processing)\n",
    "            if n_additional_blocks > 0:\n",
    "                self.additional_blocks = nn.ModuleList([\n",
    "                    ResNetBlock(d, hidden_factor, hidden_dropout) \n",
    "                    for _ in range(n_additional_blocks)\n",
    "                ])\n",
    "            else:\n",
    "                self.additional_blocks = None\n",
    "        else:\n",
    "            # Standard ResNet without DAE\n",
    "            self.input_layer = nn.Linear(input_size, d)\n",
    "            nn.init.kaiming_normal_(self.input_layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "            nn.init.constant_(self.input_layer.bias, 0)\n",
    "            \n",
    "            total_blocks = n_blocks + n_additional_blocks\n",
    "            self.blocks = nn.ModuleList([\n",
    "                ResNetBlock(d, hidden_factor, residual_dropout) \n",
    "                for _ in range(total_blocks)\n",
    "            ])\n",
    "        \n",
    "        # Prediction head (共通)\n",
    "        self.final_norm = nn.BatchNorm1d(d)\n",
    "        self.final_relu = nn.ReLU()\n",
    "        self.output = nn.Linear(d, 1)\n",
    "        nn.init.xavier_normal_(self.output.weight)\n",
    "        nn.init.constant_(self.output.bias, 0)\n",
    "    \n",
    "    def load_dae_encoder(self, dae_state_dict):\n",
    "        \"\"\"\n",
    "        Load pretrained DAE encoder weights\n",
    "        DAEのencoder部分の重みをそのまま転用\n",
    "        \"\"\"\n",
    "        if not self.use_dae_encoder:\n",
    "            raise ValueError(\"Must be initialized with use_dae_encoder=True\")\n",
    "        \n",
    "        # Extract encoder weights from DAE\n",
    "        encoder_state_dict = {}\n",
    "        for key, value in dae_state_dict.items():\n",
    "            if key.startswith('encoder.'):\n",
    "                # Remove 'encoder.' prefix\n",
    "                new_key = key.replace('encoder.', '')\n",
    "                encoder_state_dict[new_key] = value\n",
    "        \n",
    "        # Load weights into dae_encoder\n",
    "        self.dae_encoder.load_state_dict(encoder_state_dict, strict=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_dae_encoder:\n",
    "            # Use pretrained DAE encoder\n",
    "            x = self.dae_encoder(x)\n",
    "            \n",
    "            # Additional processing (optional)\n",
    "            if self.additional_blocks is not None:\n",
    "                for block in self.additional_blocks:\n",
    "                    x = block(x)\n",
    "        else:\n",
    "            # Standard ResNet\n",
    "            x = self.input_layer(x)\n",
    "            for block in self.blocks:\n",
    "                x = block(x)\n",
    "        \n",
    "        # Prediction head\n",
    "        x = self.final_norm(x)\n",
    "        x = self.final_relu(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def add_noise(x, noise_type='gaussian', noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Add noise to input data\n",
    "    \n",
    "    Args:\n",
    "        x: Input data (torch tensor)\n",
    "        noise_type: 'gaussian', 'masking', or 'swap'\n",
    "        noise_level: Noise strength\n",
    "    \"\"\"\n",
    "    if noise_type == 'gaussian':\n",
    "        # Gaussian noise\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        return x + noise\n",
    "    \n",
    "    elif noise_type == 'masking':\n",
    "        # Random masking (set some features to zero)\n",
    "        mask = torch.rand_like(x) > noise_level\n",
    "        return x * mask.float()\n",
    "    \n",
    "    elif noise_type == 'swap':\n",
    "        # Swap noise (randomly swap values)\n",
    "        noisy_x = x.clone()\n",
    "        for i in range(x.shape[1]):\n",
    "            if torch.rand(1).item() < noise_level:\n",
    "                # Shuffle this feature\n",
    "                idx = torch.randperm(x.shape[0])\n",
    "                noisy_x[:, i] = x[idx, i]\n",
    "        return noisy_x\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown noise type: {noise_type}\")\n",
    "\n",
    "\n",
    "def pretrain_dae(dae, X_train, X_val, epochs=100, batch_size=64, \n",
    "                 noise_type='gaussian', noise_level=0.1, \n",
    "                 learning_rate=1e-3, patience=20, device='cuda', seed=42, verbose=False):\n",
    "    \"\"\"\n",
    "    Pretrain DAE\n",
    "    \n",
    "    Returns:\n",
    "        best_dae_state: Best model state\n",
    "        best_val_loss: Best validation loss\n",
    "    \"\"\"\n",
    "    # Set random seeds\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    dae = dae.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(dae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # Create DataLoaders with explicit generator\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_dae_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = GradScaler('cuda') if use_amp else None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        dae.train()\n",
    "        train_loss = 0\n",
    "        for (X_batch,) in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # Add noise\n",
    "            X_noisy = add_noise(X_batch, noise_type=noise_type, noise_level=noise_level)\n",
    "            \n",
    "            # Reconstruct\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast('cuda'):\n",
    "                    X_reconstructed = dae(X_noisy)\n",
    "                    loss = criterion(X_reconstructed, X_batch)  # Reconstruct original data\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                X_reconstructed = dae(X_noisy)\n",
    "                loss = criterion(X_reconstructed, X_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_dataset)\n",
    "        \n",
    "        # Validation\n",
    "        dae.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (X_batch,) in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                X_noisy = add_noise(X_batch, noise_type=noise_type, noise_level=noise_level)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast('cuda'):\n",
    "                        X_reconstructed = dae(X_noisy)\n",
    "                        loss = criterion(X_reconstructed, X_batch)\n",
    "                else:\n",
    "                    X_reconstructed = dae(X_noisy)\n",
    "                    loss = criterion(X_reconstructed, X_batch)\n",
    "                \n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        val_loss /= len(val_dataset)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_dae_state = copy.deepcopy(dae.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"  DAE Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            if verbose:\n",
    "                print(f\"  DAE early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return best_dae_state, best_val_loss\n",
    "\n",
    "print(\"DAE classes and functions loaded successfully (ResNetBlock-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from schedulefree import RAdamScheduleFree, AdamWScheduleFree\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import copy\n",
    "\n",
    "# ============================================\n",
    "# Configuration\n",
    "# ============================================\n",
    "SEED = 42\n",
    "\n",
    "# Fixed settings\n",
    "use_c_mixup = True\n",
    "c_mixup_alpha = 1.0\n",
    "c_mixup_sigma = 1.0\n",
    "c_mixup_factor = 2\n",
    "epochs = 100000\n",
    "early_stopping_patience = 500\n",
    "\n",
    "# DAE settings\n",
    "dae_pretrain_epochs = 200\n",
    "dae_patience = 30\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ResNet for Tabular Data with DAE Support (Fixed Architecture)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"early_stopping_patience: {early_stopping_patience}\")\n",
    "print(f\"dae_pretrain_epochs: {dae_pretrain_epochs}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average (EMA) for model weights\"\"\"\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self.register()\n",
    "    \n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    \n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data\n",
    "                self.shadow[name] = new_average.clone()\n",
    "    \n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "    \n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "def smooth_clip(x, clip_val=3.0):\n",
    "    \"\"\"Apply smooth clipping using tanh\"\"\"\n",
    "    return np.tanh(x / clip_val) * clip_val\n",
    "\n",
    "\n",
    "def inverse_smooth_clip(x, clip_val=3.0):\n",
    "    \"\"\"Inverse of smooth clipping with numerical stability\"\"\"\n",
    "    x_normalized = x / clip_val\n",
    "    x_safe = np.clip(x_normalized, -0.995, 0.995)\n",
    "    result = np.arctanh(x_safe) * clip_val\n",
    "    result = np.where(np.isfinite(result), result, np.sign(x) * clip_val * 10)\n",
    "    return result\n",
    "\n",
    "\n",
    "def c_mixup(X, y, alpha=1.0, sigma=1.0, augment_factor=2):\n",
    "    \"\"\"C-Mixup (Calibrated Mixup) data augmentation\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    y_expanded = y.reshape(-1, 1)\n",
    "    label_distances = (y_expanded - y_expanded.T) ** 2\n",
    "    \n",
    "    sampling_probs = np.exp(-label_distances / (2 * sigma ** 2))\n",
    "    np.fill_diagonal(sampling_probs, 0)\n",
    "    row_sums = sampling_probs.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    sampling_probs = sampling_probs / row_sums\n",
    "    \n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    for _ in range(augment_factor):\n",
    "        for i in range(n_samples):\n",
    "            j = np.random.choice(n_samples, p=sampling_probs[i])\n",
    "            lambda_mix = np.random.beta(alpha, alpha)\n",
    "            \n",
    "            x_mix = lambda_mix * X[i] + (1 - lambda_mix) * X[j]\n",
    "            y_mix = lambda_mix * y[i] + (1 - lambda_mix) * y[j]\n",
    "            \n",
    "            X_augmented.append(x_mix)\n",
    "            y_augmented.append(y_mix)\n",
    "    \n",
    "    X_aug = np.vstack([X] + [np.array(X_augmented)])\n",
    "    y_aug = np.hstack([y] + [np.array(y_augmented)])\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Hyperparameters (新しい設計に対応)\n",
    "# ============================================\n",
    "params = {\n",
    "    # DAE settings\n",
    "    'use_dae': True,                    # DAEを使用するかどうか\n",
    "    'dae_noise_type': 'gaussian',       # ノイズタイプ: 'gaussian', 'masking', 'swap'\n",
    "    'dae_noise_level': 0.1,             # ノイズの強さ\n",
    "    'dae_lr': 1e-3,                     # DAE事前学習の学習率\n",
    "    'freeze_dae': False,                # DAEエンコーダーを固定するか\n",
    "    \n",
    "    # Model architecture (DAEとResNetで共通)\n",
    "    'd': 256,                           # モデルの次元数\n",
    "    'n_blocks': 4,                      # DAEエンコーダーのブロック数 (事前学習)\n",
    "    'n_additional_blocks': 2,           # DAEエンコーダーの後の追加ブロック数\n",
    "    'hidden_factor': 2.0,               # 隠れ層の倍率\n",
    "    'hidden_dropout': 0.1,              # 追加ブロックのDropout\n",
    "    'residual_dropout': 0.1,            # DAEエンコーダーブロックのDropout\n",
    "    \n",
    "    # Training\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': 64,\n",
    "    'optimizer': 'adamw_schedulefree',  # 'adamw', 'adamw_schedulefree', 'radam_schedulefree'\n",
    "    'loss_function': 'mae',             # 'mse', 'mae', 'smooth_l1', 'huber'\n",
    "    \n",
    "    # EMA\n",
    "    'use_ema': True,\n",
    "    'ema_decay': 0.999,\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "if params['use_dae']:\n",
    "    total_blocks = params['n_blocks'] + params['n_additional_blocks']\n",
    "    print(f\"Model structure:\")\n",
    "    print(f\"  DAE encoder: {params['n_blocks']} blocks (pretrained)\")\n",
    "    print(f\"  Additional blocks: {params['n_additional_blocks']} blocks\")\n",
    "    print(f\"  Total: {total_blocks} ResNet blocks\")\n",
    "else:\n",
    "    total_blocks = params['n_blocks'] + params['n_additional_blocks']\n",
    "    print(f\"Model structure:\")\n",
    "    print(f\"  Standard ResNet: {total_blocks} blocks\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Set random seeds\n",
    "# ============================================\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Preprocessing\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_raw.values)\n",
    "X_val_scaled = scaler_X.transform(X_val_raw.values)\n",
    "X_test_scaled = scaler_X.transform(X_test_raw.values)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_raw.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = scaler_y.transform(y_val_raw.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Smooth Clipping\n",
    "clip_val = 3.0\n",
    "X_train_clipped = smooth_clip(X_train_scaled, clip_val=clip_val)\n",
    "X_val_clipped = smooth_clip(X_val_scaled, clip_val=clip_val)\n",
    "X_test_clipped = smooth_clip(X_test_scaled, clip_val=clip_val)\n",
    "y_train_clipped = smooth_clip(y_train_scaled, clip_val=clip_val)\n",
    "y_val_clipped = smooth_clip(y_val_scaled, clip_val=clip_val)\n",
    "\n",
    "print(\"Preprocessing complete\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DAE Pretraining (if enabled)\n",
    "# ============================================\n",
    "dae_state = None\n",
    "if params['use_dae']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DAE PRETRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # DAEの初期化（新しい設計）\n",
    "    dae = DenoisingAutoencoder(\n",
    "        input_size=X_train_clipped.shape[1],\n",
    "        d=params['d'],\n",
    "        n_blocks=params['n_blocks'],\n",
    "        hidden_factor=params['hidden_factor'],\n",
    "        dropout_rate=params['residual_dropout']\n",
    "    )\n",
    "    \n",
    "    print(f\"DAE structure: {params['n_blocks']} ResNet blocks, d={params['d']}\")\n",
    "    \n",
    "    dae_state, dae_val_loss = pretrain_dae(\n",
    "        dae,\n",
    "        X_train_clipped,\n",
    "        X_val_clipped,\n",
    "        epochs=dae_pretrain_epochs,\n",
    "        batch_size=params['batch_size'],\n",
    "        noise_type=params['dae_noise_type'],\n",
    "        noise_level=params['dae_noise_level'],\n",
    "        learning_rate=params['dae_lr'],\n",
    "        patience=dae_patience,\n",
    "        device=device,\n",
    "        seed=SEED,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDAE pretraining complete. Best Val Loss: {dae_val_loss:.6f}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# C-Mixup augmentation\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA AUGMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if use_c_mixup:\n",
    "    X_train_final, y_train_final = c_mixup(\n",
    "        X_train_clipped, \n",
    "        y_train_clipped, \n",
    "        alpha=c_mixup_alpha, \n",
    "        sigma=c_mixup_sigma,\n",
    "        augment_factor=c_mixup_factor\n",
    "    )\n",
    "    print(f\"C-Mixup applied: {X_train_clipped.shape[0]} -> {X_train_final.shape[0]} samples\")\n",
    "else:\n",
    "    X_train_final = X_train_clipped\n",
    "    y_train_final = y_train_clipped\n",
    "    print(\"No augmentation\")\n",
    "\n",
    "X_val_final = X_val_clipped\n",
    "y_val_final = y_val_clipped\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Create DataLoaders\n",
    "# ============================================\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train_final, dtype=torch.float32), \n",
    "    torch.tensor(y_train_final, dtype=torch.float32)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val_final, dtype=torch.float32), \n",
    "    torch.tensor(y_val_final, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], \n",
    "                         shuffle=True, pin_memory=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], \n",
    "                       shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Initialize Model（新しい設計）\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL INITIALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if params['use_dae']:\n",
    "    # ResNetWithDAE（新しい設計）\n",
    "    model = ResNetWithDAE(\n",
    "        input_size=X_train_final.shape[1],\n",
    "        d=params['d'],\n",
    "        n_blocks=params['n_blocks'],\n",
    "        hidden_factor=params['hidden_factor'],\n",
    "        hidden_dropout=params['hidden_dropout'],\n",
    "        residual_dropout=params['residual_dropout'],\n",
    "        use_dae_encoder=True,\n",
    "        n_additional_blocks=params['n_additional_blocks']\n",
    "    )\n",
    "    \n",
    "    # Load pretrained DAE encoder\n",
    "    model.load_dae_encoder(dae_state)\n",
    "    print(f\"Loaded pretrained DAE encoder ({params['n_blocks']} blocks)\")\n",
    "    \n",
    "    # Optionally freeze DAE encoder\n",
    "    if params['freeze_dae']:\n",
    "        for param in model.dae_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"DAE encoder frozen\")\n",
    "    else:\n",
    "        print(\"DAE encoder will be fine-tuned\")\n",
    "    \n",
    "    if params['n_additional_blocks'] > 0:\n",
    "        print(f\"Additional {params['n_additional_blocks']} blocks added after DAE encoder\")\n",
    "else:\n",
    "    # Standard ResNet\n",
    "    total_blocks = params['n_blocks'] + params['n_additional_blocks']\n",
    "    model = ResNetModel(\n",
    "        input_size=X_train_final.shape[1],\n",
    "        d=params['d'],\n",
    "        n_blocks=total_blocks,\n",
    "        hidden_factor=params['hidden_factor'],\n",
    "        dropout_rate=params['residual_dropout']\n",
    "    )\n",
    "    print(f\"Standard ResNet with {total_blocks} blocks\")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model initialized on {device}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Initialize Training Components\n",
    "# ============================================\n",
    "# EMA\n",
    "use_ema = params['use_ema']\n",
    "ema = EMA(model, decay=params['ema_decay']) if use_ema else None\n",
    "if use_ema:\n",
    "    print(f\"EMA enabled with decay={params['ema_decay']}\")\n",
    "\n",
    "# Loss function\n",
    "loss_map = {\n",
    "    'mse': nn.MSELoss(), \n",
    "    'mae': nn.L1Loss(), \n",
    "    'smooth_l1': nn.SmoothL1Loss(), \n",
    "    'huber': nn.HuberLoss()\n",
    "}\n",
    "criterion = loss_map[params['loss_function']]\n",
    "print(f\"Loss function: {params['loss_function']}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer_name = params['optimizer']\n",
    "is_schedulefree = optimizer_name.endswith('_schedulefree')\n",
    "if optimizer_name == 'adamw_schedulefree':\n",
    "    optimizer = AdamWScheduleFree(model.parameters(), lr=params['learning_rate'], \n",
    "                                 weight_decay=params['weight_decay'])\n",
    "elif optimizer_name == 'radam_schedulefree':\n",
    "    optimizer = RAdamScheduleFree(model.parameters(), lr=params['learning_rate'], \n",
    "                                 weight_decay=params['weight_decay'])\n",
    "elif optimizer_name == 'adamw':\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], \n",
    "                           weight_decay=params['weight_decay'])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "print(f\"Optimizer: {optimizer_name}\")\n",
    "\n",
    "# Mixed precision training\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = GradScaler('cuda') if use_amp else None\n",
    "if use_amp:\n",
    "    print(\"Mixed precision training enabled\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training Loop\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_rmse = float('inf')\n",
    "best_model_state = None\n",
    "best_ema_shadow = None\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training mode\n",
    "    if is_schedulefree:\n",
    "        optimizer.train()\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_amp:\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs.squeeze(-1), y_batch)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(-1), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        if ema is not None:\n",
    "            ema.update()\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    \n",
    "    # Evaluation mode\n",
    "    if is_schedulefree:\n",
    "        optimizer.eval()\n",
    "    \n",
    "    if ema is not None:\n",
    "        ema.apply_shadow()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Validation\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast('cuda'):\n",
    "                    outputs = model(X_batch)\n",
    "            else:\n",
    "                outputs = model(X_batch)\n",
    "            \n",
    "            val_predictions.extend(outputs.squeeze(-1).cpu().numpy())\n",
    "            val_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    if ema is not None:\n",
    "        ema.restore()\n",
    "    \n",
    "    # Calculate RMSE in original scale\n",
    "    val_predictions_unclipped = inverse_smooth_clip(np.array(val_predictions), clip_val=clip_val)\n",
    "    val_targets_unclipped = inverse_smooth_clip(np.array(val_targets), clip_val=clip_val)\n",
    "    \n",
    "    val_predictions_original = scaler_y.inverse_transform(val_predictions_unclipped.reshape(-1, 1)).flatten()\n",
    "    val_targets_original = scaler_y.inverse_transform(val_targets_unclipped.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    val_rmse = np.sqrt(np.mean((val_predictions_original - val_targets_original)**2))\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - train_loss: {train_loss:.6f}, val_RMSE: {val_rmse:.4f} (best: {best_val_rmse:.4f})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        if ema is not None:\n",
    "            best_ema_shadow = copy.deepcopy(ema.shadow)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Load best model and make predictions\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MAKING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "if ema is not None and best_ema_shadow is not None:\n",
    "    # Apply EMA shadow for inference\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in best_ema_shadow:\n",
    "            param.data = best_ema_shadow[name]\n",
    "\n",
    "model.eval()\n",
    "test_tensor = torch.tensor(X_test_clipped, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    if use_amp:\n",
    "        with autocast('cuda'):\n",
    "            predictions_clipped = model(test_tensor).squeeze().cpu().numpy()\n",
    "    else:\n",
    "        predictions_clipped = model(test_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Inverse smooth clipping\n",
    "    predictions_unclipped = inverse_smooth_clip(predictions_clipped, clip_val=clip_val)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    predictions = scaler_y.inverse_transform(predictions_unclipped.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": range(1455, 1455 + len(predictions)), \n",
    "    \"DIC\": predictions\n",
    "})\n",
    "submission_filename = f\"submission_resnet_dae.csv\"\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Saved: {submission_filename}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# name: 坂田煌翔\n",
    "# student_id: 62408940"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7941427,
     "sourceId": 49552,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
