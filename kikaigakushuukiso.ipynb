{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "id": "w0miHB_4j6fO",
    "outputId": "2b8535a0-4018-4af3-c73c-e0f06916eba3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install schedulefree -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:35.009259Z",
     "iopub.status.busy": "2024-10-31T16:06:35.008896Z",
     "iopub.status.idle": "2024-10-31T16:06:35.040481Z",
     "shell.execute_reply": "2024-10-31T16:06:35.039428Z",
     "shell.execute_reply.started": "2024-10-31T16:06:35.009221Z"
    },
    "id": "wmcUA-Kyj6fR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
    "sub=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:35.160063Z",
     "iopub.status.busy": "2024-10-31T16:06:35.159559Z",
     "iopub.status.idle": "2024-10-31T16:06:38.637873Z",
     "shell.execute_reply": "2024-10-31T16:06:38.636808Z",
     "shell.execute_reply.started": "2024-10-31T16:06:35.160004Z"
    },
    "id": "2SYojsJUj6fS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Drop unneeded columns and handle missing values\n",
    "train = train.drop(columns=[\"Unnamed: 12\", \"id\"])  # Dropping unnecessary columns\n",
    "\n",
    "# CRITICAL FIX: Rename TA1.x to TA1 to match test data\n",
    "train = train.rename(columns={\"TA1.x\": \"TA1\"})\n",
    "\n",
    "# ============================================\n",
    "# Extract Raw Features and Target\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA PREPARATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Extract features and target\n",
    "feature_columns = [col for col in train.columns if col != 'DIC']\n",
    "X_raw = train[feature_columns].copy()\n",
    "y_raw = train['DIC'].values.copy()\n",
    "X_test_raw = test[feature_columns].copy()\n",
    "\n",
    "print(f\"元の訓練データ: {X_raw.shape}\")\n",
    "print(f\"テストデータ: {X_test_raw.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# Holdout Validation Setup\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"HOLDOUT VALIDATION SETUP\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 80% train, 20% validation\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train_raw.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val_raw.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_raw.shape[0]} samples\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:38.679793Z",
     "iopub.status.busy": "2024-10-31T16:06:38.679208Z",
     "iopub.status.idle": "2024-10-31T16:06:38.702210Z",
     "shell.execute_reply": "2024-10-31T16:06:38.701153Z",
     "shell.execute_reply.started": "2024-10-31T16:06:38.679681Z"
    },
    "id": "8kUw3FU7j6fT",
    "outputId": "a9af3f6d-335d-4a85-c2c1-bf760562a6b5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MLP Model\n",
    "# ============================================\n",
    "\n",
    "class FeatureScalingLayer(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        特徴量ごとに学習可能なスケーリング係数を持つ層\n",
    "        ソフトな特徴量選択を実現\n",
    "        \n",
    "        Args:\n",
    "            num_features: 入力特徴量の数\n",
    "        \"\"\"\n",
    "        super(FeatureScalingLayer, self).__init__()\n",
    "        # スケーリング係数を1.0で初期化（最初は元の特徴量をそのまま使用）\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 各特徴量にスケーリング係数を乗算\n",
    "        return x * self.scale\n",
    "\n",
    "\n",
    "class LearnableActivation(nn.Module):\n",
    "    def __init__(self, num_features, activation_fn):\n",
    "        \"\"\"\n",
    "        学習可能なパラメータを持つ活性化関数\n",
    "        σ_α(x) = (1-α)x + α σ(x)\n",
    "        \n",
    "        Args:\n",
    "            num_features: 特徴量の数(各ニューロンごとにαを持つ)\n",
    "            activation_fn: ベースとなる活性化関数\n",
    "        \"\"\"\n",
    "        super(LearnableActivation, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.alpha = nn.Parameter(torch.ones(num_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # αを[0, 1]の範囲にクリップ\n",
    "        alpha_clamped = torch.clamp(self.alpha, 0.0, 1.0)\n",
    "        # σ_α(x) = (1-α)x + α σ(x)\n",
    "        return (1 - alpha_clamped) * x + alpha_clamped * self.activation_fn(x)\n",
    "\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[128], dropout_rate=0.0, activation='relu', use_batchnorm=True):\n",
    "        \"\"\"\n",
    "        MLP model with flexible number of hidden layers\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_sizes: List of hidden layer sizes\n",
    "            dropout_rate: Dropout probability\n",
    "            activation: Activation function name\n",
    "            use_batchnorm: Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "\n",
    "        # Activation function mapping\n",
    "        activation_map = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'elu': nn.ELU(),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'mish': nn.Mish()\n",
    "        }\n",
    "\n",
    "        base_activation = activation_map.get(activation, nn.ReLU())\n",
    "\n",
    "        # Determine initialization based on activation\n",
    "        nonlinearity = 'relu' if activation in ['relu', 'leaky_relu'] else 'linear'\n",
    "\n",
    "        # 特徴量スケーリング層（最初の層の前に配置）\n",
    "        self.feature_scaling = FeatureScalingLayer(input_size)\n",
    "\n",
    "        # Build hidden layers dynamically\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList() if use_batchnorm else None\n",
    "        self.learnable_activations = nn.ModuleList()  # 学習可能な活性化関数\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_sizes\n",
    "        \n",
    "        for i in range(len(hidden_sizes)):\n",
    "            # Hidden layer\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            if nonlinearity == 'relu':\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "            else:\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "            self.hidden_layers.append(layer)\n",
    "            \n",
    "            # Batch normalization\n",
    "            if use_batchnorm:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(layer_sizes[i+1]))\n",
    "            \n",
    "            # 学習可能な活性化関数を各層に追加\n",
    "            self.learnable_activations.append(\n",
    "                LearnableActivation(layer_sizes[i+1], base_activation)\n",
    "            )\n",
    "            \n",
    "            # Dropout\n",
    "            self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_sizes[-1], 1)\n",
    "        nn.init.xavier_normal_(self.output.weight)\n",
    "        nn.init.constant_(self.output.bias, 0)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.shortcut = nn.Linear(input_size, 1)\n",
    "        nn.init.xavier_normal_(self.shortcut.weight)\n",
    "        nn.init.constant_(self.shortcut.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 特徴量スケーリング（最初に適用）\n",
    "        x_scaled = self.feature_scaling(x)\n",
    "        \n",
    "        # Main path\n",
    "        h = x_scaled\n",
    "        for i in range(self.num_layers):\n",
    "            h = self.hidden_layers[i](h)\n",
    "            if self.use_batchnorm:\n",
    "                h = self.batch_norms[i](h)\n",
    "            h = self.learnable_activations[i](h)  # 学習可能な活性化関数を使用\n",
    "            h = self.dropouts[i](h)\n",
    "        \n",
    "        main_output = self.output(h)\n",
    "        \n",
    "        # Residual path（スケーリングされた入力を使用）\n",
    "        residual = self.shortcut(x_scaled)\n",
    "        \n",
    "        return main_output + residual\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.optim as optim\nfrom schedulefree import RAdamScheduleFree, AdamWScheduleFree\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch.cuda.amp import autocast, GradScaler\nimport copy\n\n# Fixed hyperparameters\nactivation = 'mish'\nuse_batchnorm = False\nloss_function = 'mae'\noptimizer_name = 'adamw_schedulefree'\nbeta1 = 0.9\nbeta2 = 0.999\nclip_value = 3.0\nearly_stopping_patience = 1000\ndropout_rate = 0.0\nuse_ema = True\nema_decay = 0.999\n\n# Model and training hyperparameters\nhidden_size = 2048\nlr = 6.5e-05\nweight_decay = 1.8e-05\nbatch_size = 16\nepochs = 2500\n\nhidden_sizes = [hidden_size]\n\nprint(\"=\"*60)\nprint(\"HOLDOUT TRAINING\")\nprint(\"=\"*60)\nprint(f\"Early stopping: patience={early_stopping_patience} epochs (based on RMSE)\")\nprint(f\"\\nHyperparameters:\")\nprint(f\"  hidden_size: {hidden_size}\")\nprint(f\"  lr: {lr:.6f}\")\nprint(f\"  weight_decay: {weight_decay:.6e}\")\nprint(f\"  batch_size: {batch_size}\")\nprint(f\"  epochs: {epochs}\")\nprint(f\"  loss_function: {loss_function}\")\nprint(f\"  use_ema: {use_ema}, ema_decay: {ema_decay}\")\nprint(f\"  mixed_precision: {torch.cuda.is_available()}\")\nprint(\"=\"*60)\n\n\nclass EMA:\n    \"\"\"Exponential Moving Average (EMA) for model weights\"\"\"\n    def __init__(self, model, decay=0.999):\n        self.model = model\n        self.decay = decay\n        self.shadow = {}\n        self.backup = {}\n        self.register()\n    \n    def register(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n    \n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                new_average = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data\n                self.shadow[name] = new_average.clone()\n    \n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.backup[name] = param.data.clone()\n                param.data = self.shadow[name]\n    \n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data = self.backup[name]\n        self.backup = {}\n\n\n# Loss function\nloss_map = {'mse': nn.MSELoss(), 'mae': nn.L1Loss(), 'smooth_l1': nn.SmoothL1Loss(), 'huber': nn.HuberLoss()}\ncriterion = loss_map[loss_function]\n\n# Smooth clipping functions\ndef smooth_clip(x, clip_val=3.0):\n    return np.tanh(x / clip_val) * clip_val\n\ndef inverse_smooth_clip(x, clip_val=3.0):\n    x_clipped = np.clip(x / clip_val, -0.9999, 0.9999)\n    return np.arctanh(x_clipped) * clip_val\n\n\n# ============================================\n# Preprocessing\n# ============================================\nprint(\"\\nPreprocessing data...\")\n\n# Yeo-Johnson Transformation\nskewness_dict = {}\nfor col in X_train_raw.columns:\n    skew_val = stats.skew(X_train_raw[col])\n    skewness_dict[col] = skew_val\n\nhigh_skew_features = [col for col, skew_val in skewness_dict.items() if abs(skew_val) > 0.5]\n\nif len(high_skew_features) > 0:\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n    X_train_transformed = X_train_raw.copy()\n    X_val_transformed = X_val_raw.copy()\n    X_train_transformed[high_skew_features] = pt.fit_transform(X_train_raw[high_skew_features])\n    X_val_transformed[high_skew_features] = pt.transform(X_val_raw[high_skew_features])\nelse:\n    pt = None\n    X_train_transformed = X_train_raw\n    X_val_transformed = X_val_raw\n\n# Yeo-Johnson for target\ny_skew = stats.skew(y_train_raw)\nprint(f\"Target skewness: {y_skew:.4f}\")\n\nif abs(y_skew) > 0.5:\n    pt_y = PowerTransformer(method='yeo-johnson', standardize=False)\n    y_train_transformed = pt_y.fit_transform(y_train_raw.reshape(-1, 1)).flatten()\n    y_val_transformed = pt_y.transform(y_val_raw.reshape(-1, 1)).flatten()\n    print(f\"Applied Yeo-Johnson to target\")\nelse:\n    pt_y = None\n    y_train_transformed = y_train_raw\n    y_val_transformed = y_val_raw\n\n# Polynomial Features\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_train_expanded = poly.fit_transform(X_train_transformed.values)\nX_val_expanded = poly.transform(X_val_transformed.values)\n\n# RobustScaling + Smooth Clipping\nscaler_X = RobustScaler()\nscaler_y = RobustScaler()\n\nX_train_scaled = scaler_X.fit_transform(X_train_expanded)\nX_val_scaled = scaler_X.transform(X_val_expanded)\n\ny_train_scaled = scaler_y.fit_transform(y_train_transformed.reshape(-1, 1)).flatten()\ny_val_scaled = scaler_y.transform(y_val_transformed.reshape(-1, 1)).flatten()\n\nX_train_scaled = smooth_clip(X_train_scaled, clip_value)\nX_val_scaled = smooth_clip(X_val_scaled, clip_value)\ny_train_scaled = smooth_clip(y_train_scaled, clip_value)\ny_val_scaled = smooth_clip(y_val_scaled, clip_value)\n\n# ============================================\n# テストデータの前処理（訓練データと同じ変換）\n# ============================================\nX_test_transformed = X_test_raw.copy()\nif pt is not None and len(high_skew_features) > 0:\n    X_test_transformed[high_skew_features] = pt.transform(X_test_raw[high_skew_features])\n\nX_test_expanded = poly.transform(X_test_transformed.values)\nX_test_scaled = scaler_X.transform(X_test_expanded)\nX_test_scaled_clipped = smooth_clip(X_test_scaled, clip_value)\n\n# ============================================\n# C-Mixup Data Augmentation (Based on NeurIPS 2022 Paper)\n# ============================================\nprint(f\"\\n{'='*60}\")\nprint(\"C-MIXUP DATA AUGMENTATION\")\nprint(f\"{'='*60}\")\n\ndef c_mixup(X, y, alpha=1.0, sigma=1.0, augment_factor=2):\n    \"\"\"\n    C-Mixup (Calibrated Mixup) data augmentation based on NeurIPS 2022 paper\n    \"C-Mixup: Improving Generalization in Regression\"\n    \n    Algorithm 1 from the paper:\n    1. Calculate pairwise distance matrix P via Eqn. (6)\n    2. For each example (xi, yi):\n       - Sample (xj, yj) from P(·|(xi, yi)) and λ from Beta(α, α)\n       - Interpolate (xi, yi), (xj, yj) to get (x̃, ỹ) according to Eqn. (2)\n    \n    Args:\n        X: Feature matrix (n_samples, n_features)\n        y: Target values (n_samples,)\n        alpha: Beta distribution parameter for mixing ratio\n        sigma: Bandwidth for Gaussian kernel\n        augment_factor: Number of times to augment the dataset\n    \n    Returns:\n        X_aug: Augmented features\n        y_aug: Augmented targets\n    \"\"\"\n    n_samples = X.shape[0]\n    \n    # Step 1: Calculate pairwise distance matrix using label distances (Equation 6)\n    # d(i,j) = ||yi - yj||²₂\n    # For scalar targets: d(i,j) = (yi - yj)²\n    y_expanded = y.reshape(-1, 1)\n    label_distances = (y_expanded - y_expanded.T) ** 2  # shape (n_samples, n_samples)\n    \n    # Step 2: Calculate sampling probabilities using Gaussian kernel (Equation 6)\n    # P((xj, yj)|(xi, yi)) ∝ exp(-d(i,j)² / (2σ²))\n    sampling_probs = np.exp(-label_distances / (2 * sigma ** 2))\n    \n    # Normalize to probability mass function (sum to 1 for each row)\n    # Set diagonal to 0 to avoid sampling the same example\n    np.fill_diagonal(sampling_probs, 0)\n    row_sums = sampling_probs.sum(axis=1, keepdims=True)\n    row_sums[row_sums == 0] = 1  # Avoid division by zero\n    sampling_probs = sampling_probs / row_sums\n    \n    X_augmented = []\n    y_augmented = []\n    \n    # Step 3: Apply C-Mixup augmentation (Algorithm 1 from paper)\n    for _ in range(augment_factor):\n        for i in range(n_samples):\n            # Sample (xj, yj) according to P(·|(xi, yi))\n            j = np.random.choice(n_samples, p=sampling_probs[i])\n            \n            # Sample λ from Beta(α, α)\n            lambda_mix = np.random.beta(alpha, alpha)\n            \n            # Create mixed sample (Equation 2)\n            # x̃ = λ·xi + (1-λ)·xj\n            # ỹ = λ·yi + (1-λ)·yj\n            x_mix = lambda_mix * X[i] + (1 - lambda_mix) * X[j]\n            y_mix = lambda_mix * y[i] + (1 - lambda_mix) * y[j]\n            \n            X_augmented.append(x_mix)\n            y_augmented.append(y_mix)\n    \n    # Combine original and augmented data\n    X_aug = np.vstack([X] + [np.array(X_augmented)])\n    y_aug = np.hstack([y] + [np.array(y_augmented)])\n    \n    return X_aug, y_aug\n\n# Apply C-Mixup to training data\nprint(\"Applying C-Mixup to training data...\")\nprint(f\"Original training size: {X_train_scaled.shape[0]}\")\n\n# Hyperparameters for C-Mixup (based on paper recommendations)\nc_mixup_alpha = 1.0      # Beta distribution parameter\nc_mixup_sigma = 1.0      # Gaussian kernel bandwidth\nc_mixup_factor = 2       # Augmentation factor (2x original data)\n\nX_train_mixup, y_train_mixup = c_mixup(\n    X_train_scaled, \n    y_train_scaled, \n    alpha=c_mixup_alpha, \n    sigma=c_mixup_sigma,\n    augment_factor=c_mixup_factor\n)\n\nprint(f\"Augmented training size: {X_train_mixup.shape[0]}\")\nprint(f\"Augmentation ratio: {X_train_mixup.shape[0] / X_train_scaled.shape[0]:.1f}x\")\nprint(f\"C-Mixup parameters: alpha={c_mixup_alpha}, sigma={c_mixup_sigma}, factor={c_mixup_factor}\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\nFinal training shape: {X_train_mixup.shape}\")\nprint(f\"Final validation shape: {X_val_scaled.shape}\")\nprint(f\"{'='*60}\")\n\n# Create DataLoaders with optimization\ntrain_dataset = TensorDataset(\n    torch.tensor(X_train_mixup, dtype=torch.float32), \n    torch.tensor(y_train_mixup, dtype=torch.float32)\n)\nval_dataset = TensorDataset(\n    torch.tensor(X_val_scaled, dtype=torch.float32), \n    torch.tensor(y_val_scaled, dtype=torch.float32)\n)\n\n# Optimized DataLoader settings\ndataloader_kwargs = {\n    'batch_size': batch_size,\n    'pin_memory': True if torch.cuda.is_available() else False,\n}\n\n# Add num_workers only if not in notebook or if explicitly needed\n# Note: In Kaggle/Jupyter, num_workers > 0 can cause issues\n# Uncomment the following lines if running in a regular Python script:\n# if torch.cuda.is_available():\n#     dataloader_kwargs['num_workers'] = 2\n#     dataloader_kwargs['persistent_workers'] = True\n\ntrain_loader = DataLoader(train_dataset, shuffle=True, **dataloader_kwargs)\nval_loader = DataLoader(val_dataset, shuffle=False, **dataloader_kwargs)\n\nprint(f\"\\nDataLoader settings: {dataloader_kwargs}\")\n\n# ============================================\n# Model Training\n# ============================================\nprint(\"\\nInitializing model...\")\n\nmodel = MLPModel(\n    input_size=X_train_mixup.shape[1],\n    hidden_sizes=hidden_sizes, \n    dropout_rate=dropout_rate,\n    activation=activation,\n    use_batchnorm=use_batchnorm\n)\nmodel = model.to(device)\n\n# Initialize EMA\nema = EMA(model, decay=ema_decay) if use_ema else None\n\n# Initialize Optimizer\nis_schedulefree = optimizer_name.endswith('_schedulefree')\nif optimizer_name == 'adamw_schedulefree':\n    optimizer = AdamWScheduleFree(model.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\nelif optimizer_name == 'radam_schedulefree':\n    optimizer = RAdamScheduleFree(model.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\nelse:\n    optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\n\n# Initialize GradScaler for mixed precision training\nuse_amp = torch.cuda.is_available()\nscaler = GradScaler() if use_amp else None\n\nif use_amp:\n    print(\"Using Automatic Mixed Precision (AMP) for faster training\")\n\n# Training Loop\nbest_val_rmse = float('inf')\nbest_model_state = None\nbest_ema_shadow = None\nbest_epoch = 0\npatience_counter = 0\n\nprint(\"\\nTraining...\")\nfor epoch in range(epochs):\n    if is_schedulefree:\n        optimizer.train()\n    \n    model.train()\n    \n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Mixed precision training\n        if use_amp:\n            with autocast():\n                outputs = model(X_batch)\n                loss = criterion(outputs.squeeze(-1), y_batch)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(X_batch)\n            loss = criterion(outputs.squeeze(-1), y_batch)\n            loss.backward()\n            optimizer.step()\n        \n        if ema is not None:\n            ema.update()\n    \n    if is_schedulefree:\n        optimizer.eval()\n    \n    # Validation\n    if ema is not None:\n        ema.apply_shadow()\n    \n    model.eval()\n    \n    val_predictions = []\n    val_targets = []\n    \n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            \n            # Use AMP for validation as well\n            if use_amp:\n                with autocast():\n                    outputs = model(X_batch)\n            else:\n                outputs = model(X_batch)\n            \n            val_predictions.extend(outputs.squeeze(-1).cpu().numpy())\n            val_targets.extend(y_batch.cpu().numpy())\n    \n    if ema is not None:\n        ema.restore()\n    \n    # Calculate metrics in original scale\n    val_predictions_unclipped = inverse_smooth_clip(np.array(val_predictions), clip_value)\n    val_targets_unclipped = inverse_smooth_clip(np.array(val_targets), clip_value)\n    \n    val_predictions_original = scaler_y.inverse_transform(val_predictions_unclipped.reshape(-1, 1)).flatten()\n    val_targets_original = scaler_y.inverse_transform(val_targets_unclipped.reshape(-1, 1)).flatten()\n    \n    if pt_y is not None:\n        val_predictions_original = pt_y.inverse_transform(val_predictions_original.reshape(-1, 1)).flatten()\n        val_targets_original = pt_y.inverse_transform(val_targets_original.reshape(-1, 1)).flatten()\n    \n    val_rmse = np.sqrt(np.mean((val_predictions_original - val_targets_original)**2))\n    val_mae = np.mean(np.abs(val_predictions_original - val_targets_original))\n    \n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch + 1}/{epochs} - MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}\")\n    \n    if val_rmse < best_val_rmse:\n        best_val_rmse = val_rmse\n        best_model_state = copy.deepcopy(model.state_dict())\n        if ema is not None:\n            best_ema_shadow = copy.deepcopy(ema.shadow)\n        best_epoch = epoch + 1\n        patience_counter = 0\n    else:\n        patience_counter += 1\n    \n    if patience_counter >= early_stopping_patience:\n        print(f\"Early stopping at epoch {epoch + 1}\")\n        break\n\n# Load best model\nmodel.load_state_dict(best_model_state)\nif ema is not None and best_ema_shadow is not None:\n    ema.shadow = best_ema_shadow\n    ema.apply_shadow()\n\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"Best Validation RMSE: {best_val_rmse:.4f} at epoch {best_epoch}\")\nif use_ema:\n    print(f\"Using EMA weights (decay={ema_decay})\")\nif use_amp:\n    print(f\"Mixed precision training was enabled\")\nprint(f\"{'='*60}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Test Prediction (Holdout Method)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# テストデータは既に前処理済み (X_test_scaled_clipped)\n",
    "test_tensor = torch.tensor(X_test_scaled_clipped, dtype=torch.float32).to(device)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_scaled = model(test_tensor).squeeze().cpu().numpy()\n",
    "    predictions_unclipped = inverse_smooth_clip(predictions_scaled, clip_value)\n",
    "    predictions = scaler_y.inverse_transform(predictions_unclipped.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Apply inverse Yeo-Johnson transformation if applicable\n",
    "    if pt_y is not None:\n",
    "        predictions = pt_y.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"\\nPrediction complete!\")\n",
    "print(f\"Prediction range: [{predictions.min():.2f}, {predictions.max():.2f}]\")\n",
    "print(f\"Prediction mean: {predictions.mean():.2f}\")\n",
    "print(f\"Prediction std: {predictions.std():.2f}\")\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": range(1455, 1455 + len(predictions)), \n",
    "    \"DIC\": predictions\n",
    "})\n",
    "submission.to_csv(\"submission_holdout.csv\", index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUBMISSION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"File saved: submission_holdout.csv\")\n",
    "print(f\"Validation RMSE: {best_val_rmse:.4f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# name: 坂田煌翔\n",
    "# student_id: 62408940"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7941427,
     "sourceId": 49552,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}