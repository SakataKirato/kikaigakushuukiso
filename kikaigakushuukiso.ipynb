{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "id": "w0miHB_4j6fO",
    "outputId": "2b8535a0-4018-4af3-c73c-e0f06916eba3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install schedulefree -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:35.009259Z",
     "iopub.status.busy": "2024-10-31T16:06:35.008896Z",
     "iopub.status.idle": "2024-10-31T16:06:35.040481Z",
     "shell.execute_reply": "2024-10-31T16:06:35.039428Z",
     "shell.execute_reply.started": "2024-10-31T16:06:35.009221Z"
    },
    "id": "wmcUA-Kyj6fR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
    "sub=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:35.160063Z",
     "iopub.status.busy": "2024-10-31T16:06:35.159559Z",
     "iopub.status.idle": "2024-10-31T16:06:38.637873Z",
     "shell.execute_reply": "2024-10-31T16:06:38.636808Z",
     "shell.execute_reply.started": "2024-10-31T16:06:35.160004Z"
    },
    "id": "2SYojsJUj6fS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Drop unneeded columns and handle missing values\n",
    "train = train.drop(columns=[\"Unnamed: 12\", \"id\"])  # Dropping unnecessary columns\n",
    "\n",
    "# CRITICAL FIX: Rename TA1.x to TA1 to match test data\n",
    "train = train.rename(columns={\"TA1.x\": \"TA1\"})\n",
    "\n",
    "# ============================================\n",
    "# Feature Engineering Configuration\n",
    "# ============================================\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Mode: Polynomial Features (degree=2)\")\n",
    "print(\"  → Traditional feature engineering\")\n",
    "\n",
    "# ============================================\n",
    "# Extract Raw Features and Target\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA PREPARATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Extract features and target\n",
    "feature_columns = [col for col in train.columns if col != 'DIC']\n",
    "X_raw = train[feature_columns].copy()  # Keep raw data for SMOTER in K-Fold\n",
    "y_raw = train['DIC'].values.copy()  # Keep raw target for SMOTER in K-Fold\n",
    "X_test_raw = test[feature_columns].copy()\n",
    "\n",
    "print(f\"Raw training data: {X_raw.shape}\")\n",
    "print(f\"Raw test data: {X_test_raw.shape}\")\n",
    "print(f\"Original features: {X_raw.shape[1]} features\")\n",
    "\n",
    "# Store for use in K-Fold loop\n",
    "# We'll apply preprocessing separately for each fold after SMOTER\n",
    "\n",
    "# ============================================\n",
    "# K-Fold Cross Validation Setup\n",
    "# ============================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"K-FOLD CROSS VALIDATION SETUP\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "n_splits = 3  # Number of folds\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "print(f\"K-Fold CV: {n_splits} folds\")\n",
    "print(f\"Each fold will be trained separately\")\n",
    "print(f\"Final prediction: ensemble of all {n_splits} models\")\n",
    "print(f\"\\nNOTE: SMOTER will be applied to RAW data before preprocessing\")\n",
    "print(f\"      Validation folds will contain ONLY original samples\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-10-31T16:06:38.679793Z",
     "iopub.status.busy": "2024-10-31T16:06:38.679208Z",
     "iopub.status.idle": "2024-10-31T16:06:38.702210Z",
     "shell.execute_reply": "2024-10-31T16:06:38.701153Z",
     "shell.execute_reply.started": "2024-10-31T16:06:38.679681Z"
    },
    "id": "8kUw3FU7j6fT",
    "outputId": "a9af3f6d-335d-4a85-c2c1-bf760562a6b5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MLP Model\n",
    "# ============================================\n",
    "\n",
    "class FeatureScalingLayer(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        特徴量ごとに学習可能なスケーリング係数を持つ層\n",
    "        ソフトな特徴量選択を実現\n",
    "        \n",
    "        Args:\n",
    "            num_features: 入力特徴量の数\n",
    "        \"\"\"\n",
    "        super(FeatureScalingLayer, self).__init__()\n",
    "        # スケーリング係数を1.0で初期化（最初は元の特徴量をそのまま使用）\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 各特徴量にスケーリング係数を乗算\n",
    "        return x * self.scale\n",
    "\n",
    "\n",
    "class LearnableActivation(nn.Module):\n",
    "    def __init__(self, num_features, activation_fn):\n",
    "        \"\"\"\n",
    "        学習可能なパラメータを持つ活性化関数\n",
    "        σ_α(x) = (1-α)x + α σ(x)\n",
    "        \n",
    "        Args:\n",
    "            num_features: 特徴量の数(各ニューロンごとにαを持つ)\n",
    "            activation_fn: ベースとなる活性化関数\n",
    "        \"\"\"\n",
    "        super(LearnableActivation, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.alpha = nn.Parameter(torch.ones(num_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # αを[0, 1]の範囲にクリップ\n",
    "        alpha_clamped = torch.clamp(self.alpha, 0.0, 1.0)\n",
    "        # σ_α(x) = (1-α)x + α σ(x)\n",
    "        return (1 - alpha_clamped) * x + alpha_clamped * self.activation_fn(x)\n",
    "\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[128], dropout_rate=0.0, activation='relu', use_batchnorm=True):\n",
    "        \"\"\"\n",
    "        MLP model with flexible number of hidden layers\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_sizes: List of hidden layer sizes\n",
    "            dropout_rate: Dropout probability\n",
    "            activation: Activation function name\n",
    "            use_batchnorm: Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "\n",
    "        # Activation function mapping\n",
    "        activation_map = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'elu': nn.ELU(),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'mish': nn.Mish()\n",
    "        }\n",
    "\n",
    "        base_activation = activation_map.get(activation, nn.ReLU())\n",
    "\n",
    "        # Determine initialization based on activation\n",
    "        nonlinearity = 'relu' if activation in ['relu', 'leaky_relu'] else 'linear'\n",
    "\n",
    "        # 特徴量スケーリング層（最初の層の前に配置）\n",
    "        self.feature_scaling = FeatureScalingLayer(input_size)\n",
    "\n",
    "        # Build hidden layers dynamically\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList() if use_batchnorm else None\n",
    "        self.learnable_activations = nn.ModuleList()  # 学習可能な活性化関数\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_sizes\n",
    "        \n",
    "        for i in range(len(hidden_sizes)):\n",
    "            # Hidden layer\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            if nonlinearity == 'relu':\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "            else:\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "            self.hidden_layers.append(layer)\n",
    "            \n",
    "            # Batch normalization\n",
    "            if use_batchnorm:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(layer_sizes[i+1]))\n",
    "            \n",
    "            # 学習可能な活性化関数を各層に追加\n",
    "            self.learnable_activations.append(\n",
    "                LearnableActivation(layer_sizes[i+1], base_activation)\n",
    "            )\n",
    "            \n",
    "            # Dropout\n",
    "            self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_sizes[-1], 1)\n",
    "        nn.init.xavier_normal_(self.output.weight)\n",
    "        nn.init.constant_(self.output.bias, 0)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.shortcut = nn.Linear(input_size, 1)\n",
    "        nn.init.xavier_normal_(self.shortcut.weight)\n",
    "        nn.init.constant_(self.shortcut.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 特徴量スケーリング（最初に適用）\n",
    "        x_scaled = self.feature_scaling(x)\n",
    "        \n",
    "        # Main path\n",
    "        h = x_scaled\n",
    "        for i in range(self.num_layers):\n",
    "            h = self.hidden_layers[i](h)\n",
    "            if self.use_batchnorm:\n",
    "                h = self.batch_norms[i](h)\n",
    "            h = self.learnable_activations[i](h)  # 学習可能な活性化関数を使用\n",
    "            h = self.dropouts[i](h)\n",
    "        \n",
    "        main_output = self.output(h)\n",
    "        \n",
    "        # Residual path（スケーリングされた入力を使用）\n",
    "        residual = self.shortcut(x_scaled)\n",
    "        \n",
    "        return main_output + residual\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from schedulefree import RAdamScheduleFree, AdamWScheduleFree\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import copy\n",
    "\n",
    "# Fixed hyperparameters\n",
    "activation = 'mish'\n",
    "use_batchnorm = False\n",
    "loss_function = 'mae'\n",
    "optimizer_name = 'adamw_schedulefree'\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "use_gaussian_noise = True\n",
    "use_smoter = True\n",
    "clip_value = 3.0\n",
    "early_stopping_patience = 1000\n",
    "dropout_rate = 0.0  # Fixed at 0\n",
    "use_ema = True  # EMA (Exponential Moving Average) を使用\n",
    "ema_decay = 0.999  # EMA減衰率\n",
    "\n",
    "# Model and training hyperparameters\n",
    "hidden_size = 2048\n",
    "lr = 6.524599513245179e-05\n",
    "weight_decay = 1.7808427134495707e-05\n",
    "batch_size = 32\n",
    "noise_std = 0.01679418654429153\n",
    "noise_prob = 0.5849751972790036\n",
    "smoter_k = 6\n",
    "smoter_percentage = 0.14018900109301446\n",
    "epochs = 2500\n",
    "\n",
    "# Pseudo labeling hyperparameters\n",
    "pseudo_label_iterations = 0\n",
    "pseudo_label_percentile = 20.0\n",
    "\n",
    "hidden_sizes = [hidden_size]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"K-FOLD TRAINING WITH PSEUDO LABELING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"K-Fold: {n_splits} folds\")\n",
    "print(f\"Early stopping: patience={early_stopping_patience} epochs (based on RMSE)\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(f\"  lr: {lr:.6f}\")\n",
    "print(f\"  weight_decay: {weight_decay:.6e}\")\n",
    "print(f\"  batch_size: {batch_size}\")\n",
    "print(f\"  noise_std: {noise_std:.3f}\")\n",
    "print(f\"  noise_prob: {noise_prob:.3f}\")\n",
    "print(f\"  smoter_k: {smoter_k}\")\n",
    "print(f\"  smoter_percentage: {smoter_percentage:.2f}\")\n",
    "print(f\"  epochs: {epochs}\")\n",
    "print(f\"  pseudo_label_iterations: {pseudo_label_iterations}\")\n",
    "print(f\"  pseudo_label_percentile: {pseudo_label_percentile:.1f}%\")\n",
    "print(f\"  loss_function: {loss_function}\")\n",
    "print(f\"  use_ema: {use_ema}, ema_decay: {ema_decay}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"\n",
    "    Exponential Moving Average (EMA) for model weights\n",
    "    重みの移動平均を計算してモデルの安定性と汎化性能を向上\n",
    "    \"\"\"\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self.register()\n",
    "    \n",
    "    def register(self):\n",
    "        \"\"\"現在のモデルパラメータをshadowとして保存\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"EMA更新: shadow = decay * shadow + (1 - decay) * current\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = self.decay * self.shadow[name] + (1.0 - self.decay) * param.data\n",
    "                self.shadow[name] = new_average.clone()\n",
    "    \n",
    "    def apply_shadow(self):\n",
    "        \"\"\"推論時: EMA重みをモデルに適用\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "    \n",
    "    def restore(self):\n",
    "        \"\"\"訓練時: 元の重みを復元\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "# Loss function\n",
    "loss_map = {'mse': nn.MSELoss(), 'mae': nn.L1Loss(), 'smooth_l1': nn.SmoothL1Loss(), 'huber': nn.HuberLoss()}\n",
    "criterion = loss_map[loss_function]\n",
    "\n",
    "# Smooth clipping functions\n",
    "def smooth_clip(x, clip_val=3.0):\n",
    "    \"\"\"Smooth clipping using tanh function\"\"\"\n",
    "    return np.tanh(x / clip_val) * clip_val\n",
    "\n",
    "def inverse_smooth_clip(x, clip_val=3.0):\n",
    "    \"\"\"Inverse of smooth clipping\"\"\"\n",
    "    x_clipped = np.clip(x / clip_val, -0.9999, 0.9999)\n",
    "    return np.arctanh(x_clipped) * clip_val\n",
    "\n",
    "# Function to train K-Fold models with pseudo labels\n",
    "def train_kfold(X_original, y_original, X_pseudo=None, y_pseudo=None):\n",
    "    fold_models = []\n",
    "    fold_metrics = []\n",
    "    fold_best_epochs = []\n",
    "    \n",
    "    # K-Fold Cross Validation Loop\n",
    "    # IMPORTANT: Split ONLY on original data to prevent test data leakage\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_original)):\n",
    "        print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Get original data for this fold\n",
    "        X_train_raw_fold = X_original.iloc[train_idx].copy()\n",
    "        y_train_raw_fold = y_original[train_idx].copy()\n",
    "        X_val_raw_fold = X_original.iloc[val_idx].copy()\n",
    "        y_val_raw_fold = y_original[val_idx].copy()\n",
    "        \n",
    "        # Add pseudo labels ONLY to training fold (not validation!)\n",
    "        if X_pseudo is not None and len(X_pseudo) > 0:\n",
    "            X_train_raw_fold = pd.concat([X_train_raw_fold, X_pseudo], ignore_index=True)\n",
    "            y_train_raw_fold = np.concatenate([y_train_raw_fold, y_pseudo])\n",
    "            print(f\"  Added {len(X_pseudo)} pseudo labels to training (validation remains original)\")\n",
    "        \n",
    "        # Apply SMOTER to RAW training data\n",
    "        if use_smoter:\n",
    "            original_size = len(X_train_raw_fold)\n",
    "            n_synthetic = int(original_size * smoter_percentage)\n",
    "            \n",
    "            knn = NearestNeighbors(n_neighbors=smoter_k + 1, metric='euclidean')\n",
    "            knn.fit(X_train_raw_fold.values)\n",
    "            \n",
    "            X_synthetic = []\n",
    "            y_synthetic = []\n",
    "            \n",
    "            for _ in range(n_synthetic):\n",
    "                idx = np.random.randint(0, len(X_train_raw_fold))\n",
    "                sample_X = X_train_raw_fold.values[idx]\n",
    "                sample_y = y_train_raw_fold[idx]\n",
    "                \n",
    "                distances, indices = knn.kneighbors([sample_X])\n",
    "                neighbor_indices = indices[0][1:]\n",
    "                neighbor_idx = np.random.choice(neighbor_indices)\n",
    "                neighbor_X = X_train_raw_fold.values[neighbor_idx]\n",
    "                neighbor_y = y_train_raw_fold[neighbor_idx]\n",
    "                \n",
    "                alpha = np.random.random()\n",
    "                synthetic_X = sample_X + alpha * (neighbor_X - sample_X)\n",
    "                synthetic_y = sample_y + alpha * (neighbor_y - sample_y)\n",
    "                \n",
    "                X_synthetic.append(synthetic_X)\n",
    "                y_synthetic.append(synthetic_y)\n",
    "            \n",
    "            X_synthetic_df = pd.DataFrame(X_synthetic, columns=X_train_raw_fold.columns)\n",
    "            X_train_augmented_raw = pd.concat([X_train_raw_fold, X_synthetic_df], ignore_index=True)\n",
    "            y_train_augmented_raw = np.concatenate([y_train_raw_fold, np.array(y_synthetic)])\n",
    "        else:\n",
    "            X_train_augmented_raw = X_train_raw_fold\n",
    "            y_train_augmented_raw = y_train_raw_fold\n",
    "        \n",
    "        # Yeo-Johnson Transformation\n",
    "        skewness_dict = {}\n",
    "        for col in X_train_augmented_raw.columns:\n",
    "            skew_val = stats.skew(X_train_augmented_raw[col])\n",
    "            skewness_dict[col] = skew_val\n",
    "        \n",
    "        high_skew_features = [col for col, skew_val in skewness_dict.items() if abs(skew_val) > 0.5]\n",
    "        \n",
    "        if len(high_skew_features) > 0:\n",
    "            pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "            X_train_transformed = X_train_augmented_raw.copy()\n",
    "            X_val_transformed = X_val_raw_fold.copy()\n",
    "            X_train_transformed[high_skew_features] = pt.fit_transform(X_train_augmented_raw[high_skew_features])\n",
    "            X_val_transformed[high_skew_features] = pt.transform(X_val_raw_fold[high_skew_features])\n",
    "        else:\n",
    "            X_train_transformed = X_train_augmented_raw\n",
    "            X_val_transformed = X_val_raw_fold\n",
    "        \n",
    "        y_train_transformed = y_train_augmented_raw\n",
    "        y_val_transformed = y_val_raw_fold\n",
    "        \n",
    "        # Polynomial Features\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        X_train_expanded = poly.fit_transform(X_train_transformed.values)\n",
    "        X_val_expanded = poly.transform(X_val_transformed.values)\n",
    "        \n",
    "        # RobustScaling + Smooth Clipping\n",
    "        scaler_X = RobustScaler()\n",
    "        scaler_y = RobustScaler()\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train_expanded)\n",
    "        X_val_scaled = scaler_X.transform(X_val_expanded)\n",
    "        \n",
    "        y_train_scaled = scaler_y.fit_transform(y_train_transformed.reshape(-1, 1)).flatten()\n",
    "        y_val_scaled = scaler_y.transform(y_val_transformed.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        X_train_scaled = smooth_clip(X_train_scaled, clip_value)\n",
    "        X_val_scaled = smooth_clip(X_val_scaled, clip_value)\n",
    "        y_train_scaled = smooth_clip(y_train_scaled, clip_value)\n",
    "        y_val_scaled = smooth_clip(y_val_scaled, clip_value)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), \n",
    "                                       torch.tensor(y_train_scaled, dtype=torch.float32))\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32), \n",
    "                                     torch.tensor(y_val_scaled, dtype=torch.float32))\n",
    "        \n",
    "        g = torch.Generator().manual_seed(SEED + fold)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Initialize Model\n",
    "        model = MLPModel(\n",
    "            input_size=X_train_scaled.shape[1],\n",
    "            hidden_sizes=hidden_sizes, \n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=activation,\n",
    "            use_batchnorm=use_batchnorm\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialize EMA\n",
    "        ema = EMA(model, decay=ema_decay) if use_ema else None\n",
    "        \n",
    "        # Initialize Optimizer\n",
    "        all_params = model.parameters()\n",
    "        \n",
    "        is_schedulefree = optimizer_name.endswith('_schedulefree')\n",
    "        if optimizer_name == 'adamw_schedulefree':\n",
    "            optimizer = AdamWScheduleFree(all_params, lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'radam_schedulefree':\n",
    "            optimizer = RAdamScheduleFree(all_params, lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = optim.AdamW(all_params, lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "        \n",
    "        # Training Loop with Early Stopping\n",
    "        best_val_rmse = float('inf')\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        best_ema_shadow = None\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if is_schedulefree:\n",
    "                optimizer.train()\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                if use_gaussian_noise and np.random.rand() < noise_prob:\n",
    "                    noise = torch.randn_like(X_batch) * noise_std\n",
    "                    X_batch = X_batch + noise\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.squeeze(-1), y_batch)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update EMA after each batch\n",
    "                if ema is not None:\n",
    "                    ema.update()\n",
    "            \n",
    "            if is_schedulefree:\n",
    "                optimizer.eval()\n",
    "            \n",
    "            # Validation with EMA weights\n",
    "            if ema is not None:\n",
    "                ema.apply_shadow()  # Apply EMA weights for validation\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "            val_loss_sum = 0.0\n",
    "            val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    batch_loss = criterion(outputs.squeeze(-1), y_batch)\n",
    "                    \n",
    "                    val_loss_sum += batch_loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    val_predictions.extend(outputs.squeeze(-1).cpu().numpy())\n",
    "                    val_targets.extend(y_batch.cpu().numpy())\n",
    "            \n",
    "            if ema is not None:\n",
    "                ema.restore()  # Restore original weights for training\n",
    "            \n",
    "            val_loss = val_loss_sum / val_batches\n",
    "            val_predictions_unclipped = inverse_smooth_clip(np.array(val_predictions), clip_value)\n",
    "            val_targets_unclipped = inverse_smooth_clip(np.array(val_targets), clip_value)\n",
    "            \n",
    "            val_predictions_original = scaler_y.inverse_transform(val_predictions_unclipped.reshape(-1, 1)).flatten()\n",
    "            val_targets_original = scaler_y.inverse_transform(val_targets_unclipped.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            val_rmse = np.sqrt(np.mean((val_predictions_original - val_targets_original)**2))\n",
    "            \n",
    "            if val_rmse < best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                if ema is not None:\n",
    "                    best_ema_shadow = copy.deepcopy(ema.shadow)\n",
    "                best_epoch = epoch + 1\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        if ema is not None and best_ema_shadow is not None:\n",
    "            ema.shadow = best_ema_shadow\n",
    "            ema.apply_shadow()  # Apply best EMA weights for inference\n",
    "        \n",
    "        print(f\"  Best RMSE: {best_val_rmse:.4f} at epoch {best_epoch}\")\n",
    "        if use_ema:\n",
    "            print(f\"  Using EMA weights (decay={ema_decay})\")\n",
    "        \n",
    "        # Store for ensemble prediction\n",
    "        fold_models.append({\n",
    "            'model': model,\n",
    "            'scaler_X': scaler_X,\n",
    "            'scaler_y': scaler_y,\n",
    "            'pt': pt if len(high_skew_features) > 0 else None,\n",
    "            'high_skew_features': high_skew_features,\n",
    "            'poly': poly\n",
    "        })\n",
    "        fold_metrics.append(best_val_rmse)\n",
    "        fold_best_epochs.append(best_epoch)\n",
    "    \n",
    "    return fold_models, fold_metrics, fold_best_epochs\n",
    "\n",
    "# Pseudo labeling iterations\n",
    "X_pseudo_accumulated = None\n",
    "y_pseudo_accumulated = None\n",
    "\n",
    "for iteration in range(pseudo_label_iterations + 1):\n",
    "    if iteration == 0:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"INITIAL TRAINING (no pseudo labels)\")\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PSEUDO LABELING ITERATION {iteration}/{pseudo_label_iterations}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train K-Fold models\n",
    "    # K-Fold split is ALWAYS done on original data only\n",
    "    fold_models, fold_metrics, fold_best_epochs = train_kfold(\n",
    "        X_raw, y_raw, X_pseudo_accumulated, y_pseudo_accumulated\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nIteration {iteration} CV Results:\")\n",
    "    for i, (rmse, epoch) in enumerate(zip(fold_metrics, fold_best_epochs)):\n",
    "        print(f\"  Fold {i + 1}: RMSE {rmse:.4f} at epoch {epoch}\")\n",
    "    print(f\"  Mean CV RMSE: {np.mean(fold_metrics):.4f} ± {np.std(fold_metrics):.4f}\")\n",
    "    \n",
    "    # Pseudo labeling (skip on last iteration)\n",
    "    if iteration < pseudo_label_iterations:\n",
    "        print(f\"\\nGenerating pseudo labels for iteration {iteration + 1}...\")\n",
    "        \n",
    "        # Predict on test data with all folds\n",
    "        all_fold_predictions = []\n",
    "        for fold_idx, fold_data in enumerate(fold_models):\n",
    "            model = fold_data['model']\n",
    "            scaler_X = fold_data['scaler_X']\n",
    "            scaler_y = fold_data['scaler_y']\n",
    "            pt = fold_data['pt']\n",
    "            high_skew_features = fold_data['high_skew_features']\n",
    "            poly = fold_data['poly']\n",
    "            \n",
    "            # Preprocess test data\n",
    "            X_test_transformed = X_test_raw.copy()\n",
    "            if pt is not None and len(high_skew_features) > 0:\n",
    "                X_test_transformed[high_skew_features] = pt.transform(X_test_raw[high_skew_features])\n",
    "            \n",
    "            X_test_expanded = poly.transform(X_test_transformed.values)\n",
    "            X_test_scaled = scaler_X.transform(X_test_expanded)\n",
    "            X_test_scaled = smooth_clip(X_test_scaled, clip_value)\n",
    "            \n",
    "            test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                predictions_scaled = model(test_tensor).squeeze(-1).cpu().numpy()\n",
    "            \n",
    "            predictions_unclipped = inverse_smooth_clip(predictions_scaled, clip_value)\n",
    "            predictions = scaler_y.inverse_transform(predictions_unclipped.reshape(-1, 1)).flatten()\n",
    "            all_fold_predictions.append(predictions)\n",
    "        \n",
    "        # Calculate variance and select high-confidence samples\n",
    "        fold_std = np.std(all_fold_predictions, axis=0)\n",
    "        fold_mean = np.mean(all_fold_predictions, axis=0)\n",
    "        \n",
    "        confidence_threshold = np.percentile(fold_std, pseudo_label_percentile)\n",
    "        high_confidence_mask = fold_std < confidence_threshold\n",
    "        n_pseudo = high_confidence_mask.sum()\n",
    "        \n",
    "        print(f\"  Confidence threshold (std): {confidence_threshold:.4f}\")\n",
    "        print(f\"  High-confidence samples: {n_pseudo} ({n_pseudo/len(X_test_raw)*100:.1f}%)\")\n",
    "        \n",
    "        if n_pseudo > 0:\n",
    "            # Add pseudo labels to accumulator\n",
    "            X_pseudo_new = X_test_raw.iloc[high_confidence_mask].copy()\n",
    "            y_pseudo_new = fold_mean[high_confidence_mask]\n",
    "            \n",
    "            if X_pseudo_accumulated is None:\n",
    "                X_pseudo_accumulated = X_pseudo_new\n",
    "                y_pseudo_accumulated = y_pseudo_new\n",
    "            else:\n",
    "                X_pseudo_accumulated = pd.concat([X_pseudo_accumulated, X_pseudo_new], ignore_index=True)\n",
    "                y_pseudo_accumulated = np.concatenate([y_pseudo_accumulated, y_pseudo_new])\n",
    "            \n",
    "            print(f\"  Total pseudo labels: {len(X_pseudo_accumulated)}\")\n",
    "\n",
    "# Final CV RMSE\n",
    "mean_cv_rmse = np.mean(fold_metrics)\n",
    "std_cv_rmse = np.std(fold_metrics)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final Mean CV RMSE: {mean_cv_rmse:.4f} ± {std_cv_rmse:.4f}\")\n",
    "print(f\"Validation set: ONLY original data (no test data leakage)\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# K-Fold Ensemble Prediction with Advanced Weighting\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"K-FOLD ADVANCED ENSEMBLE PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of models: {len(fold_models)}\")\n",
    "print(f\"Weighting: Adaptive per-sample confidence + diversity bonus\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================\n",
    "# Step 1: Calculate global fold weights from validation RMSE\n",
    "# ============================================\n",
    "fold_rmse_array = np.array(fold_metrics)\n",
    "# 三乗逆数を使用（より性能差を強調）\n",
    "global_weights = 1.0 / (fold_rmse_array ** 3)\n",
    "global_weights = global_weights / global_weights.sum()\n",
    "\n",
    "print(f\"\\nGlobal fold weights (based on validation RMSE²):\")\n",
    "for i, (rmse, weight) in enumerate(zip(fold_metrics, global_weights)):\n",
    "    print(f\"  Fold {i + 1}: RMSE={rmse:.4f} → Global Weight={weight:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 2: Collect predictions and calculate diversity\n",
    "# ============================================\n",
    "all_fold_predictions = []\n",
    "\n",
    "for fold_idx, fold_data in enumerate(fold_models):\n",
    "    print(f\"\\nFold {fold_idx + 1}/{len(fold_models)} predicting...\")\n",
    "    \n",
    "    # Extract fold components\n",
    "    model = fold_data['model']\n",
    "    scaler_X = fold_data['scaler_X']\n",
    "    scaler_y = fold_data['scaler_y']\n",
    "    pt = fold_data['pt']\n",
    "    high_skew_features = fold_data['high_skew_features']\n",
    "    poly = fold_data['poly']\n",
    "    \n",
    "    # Apply same preprocessing as training\n",
    "    X_test_transformed = X_test_raw.copy()\n",
    "    if pt is not None and len(high_skew_features) > 0:\n",
    "        X_test_transformed[high_skew_features] = pt.transform(X_test_raw[high_skew_features])\n",
    "    \n",
    "    X_test_expanded = poly.transform(X_test_transformed.values)\n",
    "    X_test_scaled = scaler_X.transform(X_test_expanded)\n",
    "    X_test_scaled = smooth_clip(X_test_scaled, clip_value)\n",
    "    \n",
    "    test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Single prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_scaled = model(test_tensor).squeeze().cpu().numpy()\n",
    "        predictions_unclipped = inverse_smooth_clip(predictions_scaled, clip_value)\n",
    "        predictions = scaler_y.inverse_transform(predictions_unclipped.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    all_fold_predictions.append(predictions)\n",
    "    print(f\"Fold {fold_idx + 1} complete - Prediction range: [{predictions.min():.2f}, {predictions.max():.2f}]\")\n",
    "\n",
    "all_fold_predictions = np.array(all_fold_predictions)  # shape: (n_folds, n_samples)\n",
    "\n",
    "# ============================================\n",
    "# Step 3: Calculate per-sample adaptive weights\n",
    "# ============================================\n",
    "print(f\"\\nCalculating adaptive per-sample weights...\")\n",
    "\n",
    "# サンプルごとの標準偏差（予測の不一致度）\n",
    "sample_std = np.std(all_fold_predictions, axis=0)  # shape: (n_samples,)\n",
    "\n",
    "# サンプルごとの信頼度重み\n",
    "# 標準偏差が小さい（モデル間の一致度が高い）ほど信頼度が高い\n",
    "confidence_scores = np.exp(-sample_std / np.mean(sample_std))  # shape: (n_samples,)\n",
    "\n",
    "# 各フォールドごとのサンプル別適応重み\n",
    "adaptive_weights = np.zeros_like(all_fold_predictions)  # shape: (n_folds, n_samples)\n",
    "\n",
    "for fold_idx in range(len(fold_models)):\n",
    "    # 各サンプルについて、このフォールドの予測が平均からどれだけ離れているか\n",
    "    mean_pred = np.mean(all_fold_predictions, axis=0)\n",
    "    deviation = np.abs(all_fold_predictions[fold_idx] - mean_pred)\n",
    "    \n",
    "    # 偏差が小さいほど（コンセンサスに近いほど）重みが大きい\n",
    "    consensus_weight = np.exp(-deviation / (np.std(deviation) + 1e-8))\n",
    "    \n",
    "    # グローバル重み × コンセンサス重み × 信頼度スコア\n",
    "    adaptive_weights[fold_idx] = global_weights[fold_idx] * consensus_weight * confidence_scores\n",
    "\n",
    "# 各サンプルで正規化（合計が1になるように）\n",
    "adaptive_weights = adaptive_weights / (adaptive_weights.sum(axis=0, keepdims=True) + 1e-8)\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Apply adaptive weighted ensemble\n",
    "# ============================================\n",
    "final_predictions = np.sum(all_fold_predictions * adaptive_weights, axis=0)\n",
    "\n",
    "# ============================================\n",
    "# Step 5: Post-processing with diversity consideration\n",
    "# ============================================\n",
    "# 標準偏差が非常に大きいサンプル（不確実性が高い）には中央値を使用\n",
    "high_uncertainty_mask = sample_std > np.percentile(sample_std, 99)\n",
    "median_predictions = np.median(all_fold_predictions, axis=0)\n",
    "\n",
    "if high_uncertainty_mask.sum() > 0:\n",
    "    print(f\"\\nApplying median for high-uncertainty samples: {high_uncertainty_mask.sum()} samples\")\n",
    "    final_predictions[high_uncertainty_mask] = median_predictions[high_uncertainty_mask]\n",
    "\n",
    "# Calculate statistics\n",
    "fold_std = np.std(all_fold_predictions, axis=0)\n",
    "avg_adaptive_weight_std = np.std(adaptive_weights, axis=1).mean()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ADVANCED ENSEMBLE COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final predictions - Min: {final_predictions.min():.2f}, Max: {final_predictions.max():.2f}, Mean: {final_predictions.mean():.2f}\")\n",
    "print(f\"Average std across folds: {fold_std.mean():.4f}\")\n",
    "print(f\"High confidence samples (std < 1.0): {(sample_std < 1.0).sum()} / {len(sample_std)}\")\n",
    "print(f\"Low confidence samples (std > 2.0): {(sample_std > 2.0).sum()} / {len(sample_std)}\")\n",
    "print(f\"\\nWeighting summary:\")\n",
    "print(f\"  Global weight range: [{global_weights.min():.4f}, {global_weights.max():.4f}]\")\n",
    "print(f\"  Adaptive weight variation: {avg_adaptive_weight_std:.4f}\")\n",
    "print(f\"  Median-adjusted samples: {high_uncertainty_mask.sum()}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\"id\": range(1455, 1455 + len(final_predictions)), \"DIC\": final_predictions})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSubmission saved to submission.csv!\")\n",
    "print(f\"CV RMSE estimate: {np.mean(fold_metrics):.4f} ± {np.std(fold_metrics):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score: 3.68794\n",
    "# name: 坂田煌翔\n",
    "# student_id: 62408940"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7941427,
     "sourceId": 49552,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
